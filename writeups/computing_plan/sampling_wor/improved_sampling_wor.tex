\documentclass[12pt]{article}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{float}
\usepackage[caption = false]{subfig}
\usepackage{/Users/timbarry/Documents/optionFiles/mymacros}

\begin{document}
\noindent
Tim B.

\begin{center}
\textbf{Marginal sampling within \texttt{sceptre}}
\end{center}

\subsection*{Definition and motivation}

I describe ``inductive sampling without replacement,'' a procedure for sharing without replacement (WOR) samples across gene-gRNA pairs containing the same number of negative control (NT) cells but different numbers of treatment cells. Inductive sampling WOR reduces the number of WOR samples that must drawn by a factor equal to the number of gRNAs in the dataset. On a more technical level inductive sampling WOR considerably reduces the number of database queries (specifically, gene expression vector loads) that must be issued when running SCEPTRE out-of-core. Inductive sampling WOR therefore could reduce compute by several orders of magnitude.

Let $N$ be the number of control cells. Label the control cells by $c_1, c_2, \dots, c_N$. Next, let $M$ be the number of treatment cells containing the the gRNA that infects the greatest number of cells. Label the treatment cells by $t_1, \dots, t_M$. We seek to construct a length-$M$ random sequence $a_1, a_2, \dots, a_M$ that satisfies the following properties:
\begin{itemize}
\item[1.] $a_i \in \{ c_1, \dots, c_N, t_1, \dots, t_i \} $ for all $a_i$ (i.e., the $i$th element of the sequence is a control cell or one of the first $i$ treatment cells).
\item[2.] $a_i \neq a_j$ (i.e., the elements of the sequence are unique).
\item[3.] Letting $A_i$ denote the set containing the first $i$ elements of the sequence (i.e., $A_i := \{ a_1, a_2, \dots, a_i \}$), then
\begin{multline*}
\P(c_1 \in A_i) = \P(c_2 \in A_i) = \dots = \P(c_N \in A_i) = \\ \P(t_1 \in A_i) = \dots = \P(t_i \in A_i) = \frac{i}{i + N}.
\end{multline*}
\end{itemize}

In other words, we seek to construct a sequence of increasing sets $A_1 \subset A_2 \subset \dots \subset A_M$ that satisfies an ``inductive'' WOR sampling property. The first set $A_1$, which contains a single element, contains the control cells and the first treatment cell with equal probability. The second set $A_2$, which contains two elements, contains the control cells and each of the first \textit{two} treatment cells with equal probability, and so on. This property is appealing because it enables us to share random samples across gRNAs. If a given gRNA has $i$ treatment cells, then $A_i$ is a valid WOR sample for this gRNA. Thus, we need only generate a random sequence $a_1 \dots a_M$ once and share this random sequence across all gRNAs.

\subsection*{Constructing an inductive WOR sample}

I describe a strategy for constructing an inductive WOR sample, and I prove its correctness. I describe the procedure inductively.

\textbf{Step 1}. Sample one element from the set $a_1 \leftarrow \{c_1, \dots, c_N, t_1\},$ putting an equal mass of  $1/(N + 1)$ onto each of the elements.

\textbf{Step $i$, for $i \geq 2$}. Let $B_i := \{c_1, \dots, c_N, t_1 \dots, t_{i-1} \} \setminus A_{i-1}$ be the set of ``leftover'' elements that were not selected in step $i-1$. There are $N$ elements in the set $B_i$. Draw an element at random from the set $B_i \bigcup \{ t_i \}$, placing a mass of $\frac{i}{i + N}$ on $t_i$ and a mass of $\left(1 - \frac{i}{i + N} \right)/N$ on each of the elements in $B_i$. Set the sampled element to $a_i$. Continue this process until $i = M$, resulting in a sequence $a_1,\dots,a_M$. This sequence satisfies the desired property stated above, which I now prove.

\textbf{Proof}. I proceed inductively. \underline{Base case}: Let $i = 1$. Then $\P(c_1 \in A_1) = \dots \P(c_N \in A_1) = \P(t_1 \in A_1) = 1/(1+N).$ Next, let $i \in \{2, \dots, M \}$ be given. \underline{Inductive step}: Suppose that 
\begin{multline*}
\P(c_1 \in A_{i}) = \dots = \P(c_N \in A_{i}) = \P( t_1 \in A_{i} ) = \dots \\ = \P(t_{i} \in A_{i}) = \frac{i}{N + i}.
\end{multline*}
We construct $a_{i+1}$ by sampling $t_{i+1}$ with probability $(i+1)/(N + i + 1)$ and the elements of $B_i$ with probability $\left(1 - \frac{i + 1}{N+i + 1}\right)/N$. We have by construction that $$\P(t_{i+1} \in A_{i+1}) = \frac{i+1}{N+i+1}.$$ Next, let $u \in \{c_1, \dots, c_N, t_1, \dots, t_{i} \}$. We compute $\P(u \in A_{i+1})$ using the law of total probability:
\begin{equation}\label{key_eq}
\P(u \in A_{i+1}) = \P(u \in A_{i+1} | u \in A_{i}) \P(u \in A_{i}) + \P(u \in A_{i+1} | u \notin A_{i}) \P(u \notin A_{i}).
\end{equation}
Considering first the lefthand term, we have that $\P(u \in A_{i+1} | u \in A_{i}) = 1,$ as membership in $A_{i}$ implies membership in $A_{i+1}$. Next, $\P(u \in A_{i}) = i/(N + i)$ by the inductive hypothesis. Thus, the left term of (\ref{key_eq}) is
$i/(N + i).$ Next, consider the righthand term. If $u \notin A_{i}$, then $u \in B_k$. Thus, $$ \P(u \in A_{i+1} | u \notin A_{i}) = \left(1 - \frac{i+1}{N+1+i}\right)/N.$$ Furthermore, by the inductive hypothesis, $\P(u \notin A_{i-1}) = 1 - i/(N + i)$. Stringing these pieces together,

\begin{multline*}
\P \left( u \in A_{i+1} \right) = \frac{i}{N + i} + \frac{1}{N} \left(1 - \frac{i+1}{N+1+i}\right) \left( 1 - \frac{i}{N+i} \right) \\ = \frac{i}{N+i} + \frac{1}{N} \left( \frac{N+1+i}{N+1+i} - \frac{i+1}{N+1+i} \right)\left( \frac{N+i}{N+i} - \frac{i}{N+i} \right) \\ = \frac{i}{N+i} + \frac{1}{N} \left( \frac{N}{N+1+i} \right) \left( \frac{N}{N+i} \right) = \frac{i}{N+i} + \left( \frac{1}{N+1+i} \right)\left( \frac{N}{N+i} \right) \\ = \frac{i}{N+i} + \frac{N}{(N+1+i)(N+i)} = \frac{i(N+1+i) + N}{(N+i)(N+1+i)} = \frac{iN + i + i^2 + N}{(N+1)(N+1+i)} \\ = \frac{(N+i)(i+1)}{(N+1)(N+1+i)} = \frac{i+1}{N+1+i}. \\ \square
\end{multline*}


\subsection*{An algorithm for inductive WOR sampling}

I describe an algorithm for inductive WOR sampling using only a $U(0,1)$ random number generator. First, I introduce a distribution --- the ``IWOR distribution.'' The $\text{IWOR}(N, i)$ distribution is a discrete probability distribution that has support $\{ 0, \dots, N \}$ and places mass $i/(i + N)$ on $N$ and mass $\frac{1}{N}\left[1 - i/(i+1)\right]$ on $\{0, 1, \dots, N - 1\}.$ One can sample from the $\text{IWOR}(N,i)$ distribution as follows (Algorithm \ref{alg:iwor_dist}). The algorithm is fast, requiring only an if statement, a multiplication, and a floor operation.

Next, I give an algorithm for constructing an inductive WOR sequence $a_1 a_2 \dots a_M$ given negative control cells $c_1, \dots c_N$ and treatment cells $t_1, \dots, t_M$ (Algothtm \ref{alg:iwor_sample}; the algorithm uses zero-based indexing.) The algorithm is efficient, requiring $O(M)$ time and $O(N + M)$ space. In practice we will index the control cells by $0, 1, \dots, N-1$ and the treatment cells by $N, N+1, \dots, N + M - 1$. Thus the initialization of $r$ step (namely, line 2) can be rewritten as $$ r \leftarrow [0, 1, \dots, N - 1, N],$$ and the final line of the for loop can be rewritten as
$$ r[N] \leftarrow N + i - 1.$$

Suppose we want to construct a sequence $a_1 a_2 \dots a_{m-1} a_m a_{m+1} \dots a_{M-1}a_M$ that satisfies the inductive WOR property from $m$ to $M$. (In our application $m$ and $M$ may be the minimum and maximum number of treatment cells, respectively). We can construct $a_1\dots a_{m-1}$ via standard sampling WOR and then run Algorithm \ref{alg:iwor_sample} to construct $a_m\dots a_M$. A common algorithm for standard WOR sampling is the Fisher-Yates sampler. 

\begin{algorithm}
	\caption{Sampling from the $\textrm{IWOR}(N,i)$ distribution.}\label{alg:iwor_dist}
	\begin{algorithmic}
		\Require $N, i$ 
		\State $u \sim U(0,1)$
		\State $p \leftarrow i/(N+i)$
		\If{$u > 1 - p$}
		\State $d \leftarrow N$
		\Else 
		\State $d \leftarrow \lfloor uN/(1-p) \rfloor $ // floor operator
		\EndIf
		\State \textbf{return} $d$
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}
	\caption{Constructing an inductive WOR sample.}\label{alg:iwor_sample}
	\begin{algorithmic}
		\Require Control cells $c_1, \dots, c_N$ and treatment cells $t_1, \dots, t_M$.
		\State Initialize $v \leftarrow \textrm{vector}(M).$
		\State Initialize $r \leftarrow [ c_1, c_2, \dots, c_N, t_1 ].$
		\For{i = 1 $\dots$ M }
		\State $\texttt{pos} \sim \textrm{IWOR}(N, i)$ // sample a position within $v$
		\State $v[i-1] \leftarrow r[\texttt{pos}] $ // extract the element at that position
		\State $ r[\texttt{pos}] \leftarrow r[N]$ // move the rightmost entry of $r$ to position \texttt{pos}
		\State $r[N] \leftarrow t_{i+1}$ // update the rightmost entry of $r$ with $t_{i+1}$
		\EndFor
		\State \textbf{return} $v$
	\end{algorithmic}
\end{algorithm}


\begin{algorithm}
\caption{Hybrid Fisher-Yates/IWOR sampler}
\begin{algorithmic}
\Require Control cells $c_1, \dots, c_N$ and treatment cells $t_1, \dots, t_M$; the minimum number of treatment cells $m$.
\State $x \leftarrow [c_1, \dots, c_N, t_1, \dots, t_m].$
\State $v \leftarrow \textrm{vector}(M)$ 
\For{$i = 1, \dots, m$} // Fisher-Yates step:
\State $u \sim \textrm{Unif}(\{0, 1, \dots, N + m - i\})$
\State $\textrm{Swap}(x[N + m - i], x[u])$
\EndFor
\State // entries $x[0, \dots, N - 1]$ are $N$ leftovers from $x$
\State // entries $x[N, \dots, N + m - 1]$ are $m$ samples WOR from $x$
\\
\For{$i = 0, \dots, m - 1$}
\State $v[i] \leftarrow x[i + N]$
\EndFor

\\ 
\State $x[N] \leftarrow t_{m+1}$ // initialize inductive WOR step
\For{i = $m+1, \dots, M$}
\State \texttt{pos} $\sim$ \text{IWOR}(N, i)
\State $v[i - 1] \leftarrow x[\texttt{pos}]$
\State $x[\texttt{pos}] \leftarrow x[N]$
\State $x[N] \leftarrow t_{i+1}$
\EndFor
\State \textbf{return} $v$
\end{algorithmic}
\end{algorithm}

\subsection*{Sampling without replacement for the undercover analysis}

The undercover analysis differs from the discovery analysis in low MOI. In the undercover analysis the total number of cells remains the same, while the number of treatment and control cells varies. Index the negative control cells by $\{1, \dots, N\}$. Denote the number of cells in the largest undercover group by $M$. We seek to construct a length-$M$ sequence of indices with the following property:

\begin{itemize}
\item[1.] $a_i \in \{1, \dots, N\}$ for all $a_i$. (Note: there is no restriction on the specific values that $a_i$ can take as a function of $i$; instead, $a_i$ can be any member of $ \{1, \dots, N \}$).
\item[2.] $a_i \neq a_j$.
\item[3.] Let $A_i = \{ a_1, \dots, a_i \}.$ Then
$$ \P( 1 \in A_i ) = \P(1 = A_i) = \dots = \P(N \in A_i ) = i/N.$$
\end{itemize}

We can use a length-M sequence $a_1 a_2 \dots a_M$ to 

\begin{table}
\begin{tabular}{cccc}
	\hline 
	\textbf{MOI} & \textbf{Sampling mechanism} & \textbf{Analysis type} & \textbf{Sampling algorithm} \\ 
	\hline 
	Low & Marginal & Disovery &  Inductive WOR \\
	\hline 
	Low & Marginal & Undercover & Standard WOR \\ 
	\hline 
	Low & Conditional & Discovery &  ? \\ 
	\hline 
	Low & Conditional & Undercover & ? \\ 
	\hline 
	High & Marginal & Discovery & Standard WOR \\ 
	\hline 
	High & Marginal & Undercover & Standard WOR  \\ 
	\hline 
	High & Conditional & Discovery & ? \\ 
	\hline 
	High & Conditional & Undercover & ? \\ 
	\hline 
\end{tabular}
	\caption{Resampling algorithms to use in different settings. The inductive WOR algorithm is used only in the following setting: low MOI, marginal resampling, discovery analysis. It is unclear what algorithm to use when resampling \textit{conditional} (as opposed to \textit{marginally}).}
\end{table}

\bibliographystyle{unsrt}
\bibliography{/Users/timbarry/Documents/optionFiles/library.bib}

\end{document}
