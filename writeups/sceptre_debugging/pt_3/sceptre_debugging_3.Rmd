---
title: 'SCEPTRE debugging part 3'
author: "Tim"
date: "2022-10-10"
output: html_document
---

```{r, message = FALSE, echo = FALSE}
# load packages
library(tidyverse)
library(katlabutils)
result_dir <- paste0(.get_config_path("LOCAL_SCEPTRE2_DATA_DIR"), "results/")

# load the auxilliary functions
funct_script <- paste0(.get_config_path("LOCAL_CODE_DIR"), "sceptre2-manuscript/writeups/undercover_results_eda/analyze_undercover_results_plot_functs.R")
source(funct_script)

# 1. Fisher exact test negative control results
fisher_exact_res <- readRDS(paste0(result_dir, "undercover_grna_analysis/fisher_exact_1.rds"))
# 2. Positive control results
pc_res <- readRDS(paste0(result_dir, "positive_control_analysis/pc_results.rds"))
# 3. sample size information df
sample_size_df <- readRDS(paste0(result_dir, "dataset_sample_sizes/n_nonzero_cells_per_grna.rds")) |>
  dplyr::mutate(dataset = dataset_concat,
                dataset_concat = NULL, paper = NULL, modality = NULL) |>
  dplyr::rename(grna_group = target, response_id = feature_id)
# 4. undercover results (excluding Fisher) for group size = 1
undercover_res <- readRDS(paste0(result_dir, "undercover_grna_analysis/undercover_result_grp_size_1.rds")) |>
  dplyr::select(-clock_time, -max_ram) |>
  dplyr::filter(method %in% c("seurat_de", "liscovitch_method", "schraivogel_method", "weissman_method", "mimosca", "nb_regression"))
# 5. undercover result sceptre (rbind with above undercover results)
undercover_res_sceptre <- readRDS(paste0(result_dir, "undercover_grna_analysis/undercover_result_grp_size_1_sceptre.rds")) |>
  dplyr::select(-clock_time, -max_ram)
undercover_res <- rbind(undercover_res, undercover_res_sceptre, fisher_exact_res)
# 6. singleton positive control results
pc_res_singleton <- readRDS(paste0(result_dir, "positive_control_analysis/pc_results_singleton.rds")) |> rename("grna_id" = "grna_group")
# 7. sceptre debugging results
sceptre_debug <- readRDS(paste0(result_dir, "undercover_grna_analysis/sceptre_debug.rds"))
# 8. sceptre / NB results with leading PCs as covariates
sceptre_nb_pca_res <- readRDS(paste0(result_dir, "undercover_grna_analysis/sceptre_nb_pca_1.rds"))
# 9. 
mw_debug <- readRDS(paste0(result_dir, "undercover_grna_analysis/mw_debug.rds"))
```

```{r, message = FALSE, echo = FALSE}
# Combine the results of the Schraivogel datasets
replace_slash_w_underscore <- function(df) {
  df |> dplyr::mutate(dataset = gsub(pattern = "/",
                                replacement = "_",
                                fixed = TRUE,
                                x = dataset))
}

update_df <- function(df) {
  df |>
    replace_slash_w_underscore() |>
    combine_schraivogel_enhancer_screens() |>
    update_dataset_names(add_n_pairs = TRUE)
}

sample_size_df <- sample_size_df |>
  replace_slash_w_underscore() |>
  combine_schraivogel_enhancer_screens() 
pc_res <- update_df(pc_res)
pc_res_singleton <- update_df(pc_res_singleton)
undercover_res <- update_df(undercover_res)
fisher_exact_res <- update_df(fisher_exact_res)
ks_stat_categories <- list(good = c(0, 1e-2), adequate = c(1e-2, 5e-2), subpar = c(5e-2, 0.1), poor = c(0.1, 1))
sceptre_debug <- update_df(sceptre_debug) |>
  mutate(ks_fit = cut(x = ks_stat,
                      breaks = unique(unlist(ks_stat_categories)),
                      labels = names(ks_stat_categories)))
sceptre_nb_pca_res <- sceptre_nb_pca_res |>
  replace_slash_w_underscore() |>
  update_dataset_names()
```

This is the third writeup in the SCEPTRE debugging series. The broad purpose of this series is to answer two overarching questions: (1) what are the analysis challenges, and (2) how can we address them? To this end I ask four more specific questions in this writeup.

1. To what extent does applying a more stringent QC threshold *actually* make the problem easier? (Answer: Applying more stringent QC makes the problem easier on the Frangieh data but not on the Papalexi or Schraivogel data.)

2. Do the Seurat resampling distributions resemble the SCEPTRE resampling distributions in easy and challenging problem settings? (In progress.)

3. Might latent confounding explain the miscalibration we observe on the Schraivogel perturb-seq data? (Answer: Not really.)

4. How do the *exact* (as opposed to *skew-normal*) SCEPTRE p-values look? Are they well-calibrated? (Answer: the exact SCEPTRE p-values and skew-normal SCEPTRE p-values coincide on the Papalexi and Schraivogel data but diverge on the Frangieh data.)

Throughout, I use a modified version of the Fisher exact test in which I partition the cells into "highly" and "lowly" expressed groups based on the median expression of the gene. I then construct a contigency table based on this categorization.

# Does applying more stringent QC actually make the problem easier?

First, recall that the calibration results for SCEPTRE, Seurat DE, and Fisher exact test look as follows:

```{r, message = FALSE, echo = FALSE, fig.height = 8, fig.width = 8}
datasets_to_keep <- c("frangieh_co_culture_gene",
                        "papalexi_eccite_screen_gene",
                        "schraivogel_enhancer_screen",
                        "schraivogel_ground_truth_tapseq_gene",
                        "frangieh_control_gene",
                        "frangieh_ifn_gamma_gene",
                        "schraivogel_ground_truth_perturbseq_gene")

p <- undercover_res |>
  filter(dataset %in% datasets_to_keep,
         method %in% c("seurat_de", "fisher_exact", "sceptre")) |>
ggplot(mapping = aes(y = p_value, col = Method)) +
   stat_qq_points(ymin = 1e-9, size = 0.8) +
   facet_wrap(~dataset_rename_w_pairs, scales = "free", ncol = 3, labeller = label_wrap_gen(35)) +
   geom_abline(col = "darkred") +
   stat_qq_band() +
   theme_bw() +
   scale_x_continuous(trans = revlog_trans(10)) +
   scale_y_continuous(trans = revlog_trans(10)) +
   labs(x = "Expected quantile", y = "Observed quantile") +
   theme(legend.position = "bottom",
         panel.grid.major = element_blank(),
         panel.grid.minor = element_blank(),
         panel.background = element_blank())
p
```

In the previous writeup we saw that applying more stringent QC appears to improve the calibration of all methods across all datasets. However, using a more stringent QC threshold reduces the total number of pairs plotted, thereby preventing us from looking far out into the tail of the distribution. Thus, the apparent improvement in calibration might be a result of the reduction in the number of pairs plotted rather than an actual improvement in calibration.

We thefore seek to asses whether there is a true change in calibration that results from increasing the QC threshold (controlling for the number of pairs plotted). To this end, for each method, we create a QQ-plot that consists of two sets of pairs: (i) the *N* pairs that survive the QC threshold, and (ii) *N* pairs randomly sampled (without replacement) from the full set of pairs. If these two sets of pairs differ in calibration quality --- and in particular, if the former set of pairs exhibits better calibration than the latter --- then we can conclude that QC actually does make the problem easier. The QC that we use entails filtering for pairs with an effective sample size of 250 cells or greater.

```{r, message = FALSE, echo = FALSE, fig.height = 8, fig.width = 8}
qc_thresh <- 250

ntc_effective_samp_size <- sample_size_df |>
  filter(grna_group == "non-targeting") |>
  group_by(response_id, dataset) |>
  summarize(effective_samp_size = sum(n_nonzero_cells))

undercover_res_sub <- undercover_res |>
  left_join(ntc_effective_samp_size, by = c("response_id", "dataset")) |>
  filter(method %in% c("seurat_de", "sceptre", "fisher_exact"),
         dataset %in% datasets_to_keep)

undercover_res_sub_qc <- undercover_res_sub |>
  filter(effective_samp_size >= qc_thresh) |>
  mutate(pair_type = "QC")

n_after_qc <- undercover_res_sub_qc |>
    group_by(method, dataset) |>
    summarize(count = n()) |>
    filter(method == "seurat_de") |>
    select(dataset, count)

undercover_res_sub_downsample <- 
  undercover_res_sub |>
  group_by(method, dataset) |>
  group_modify(.f = function(tbl, key) {
    curr_method <- as.character(key$method)
    curr_dataset <- as.character(key$dataset)
    n_to_draw <- n_after_qc |>
      filter(dataset == curr_dataset) |>
      pull(count)
    out <- sample_n(tbl, size = n_to_draw, replace = FALSE)
}) |> ungroup() |> mutate(pair_type = "Downsample")

to_plot <- rbind(undercover_res_sub_qc,
                 undercover_res_sub_downsample) |> update_dataset_names(TRUE)

rm(n_after_qc); rm(undercover_res_sub_downsample); rm(undercover_res_sub); rm(undercover_res_sub_qc)
```

First, we plot the results for the Fisher exact test.

```{r, fig.align = "center", message=FALSE, warning=FALSE, echo=FALSE, fig.height = 8, fig.width = 8}
p <- ggplot(data = to_plot |> filter(method == "fisher_exact"),
             mapping = aes(y = p_value, col = pair_type)) +
   stat_qq_points(ymin = 1e-9, size = 0.8) +
   facet_wrap(~dataset_rename_w_pairs, scales = "free", ncol = 3, labeller = label_wrap_gen(35)) +
   geom_abline(col = "darkred") +
   stat_qq_band() +
   theme_bw() +
   scale_x_continuous(trans = revlog_trans(10)) +
   scale_y_continuous(trans = revlog_trans(10)) +
   labs(x = "Expected quantile", y = "Observed quantile") +
   theme(legend.position = "bottom",
         panel.grid.major = element_blank(),
         panel.grid.minor = element_blank(),
         panel.background = element_blank()) +
  scale_color_manual(values = c("chartreuse4", "dodgerblue3")) +
  labs(color = "Pair subset")
p
```

We do not observe a major difference between the QC'ed pairs and the randomly downsampled pairs in any of the plots. Next, we plot the results for Seurat DE.

```{r, fig.align = "center", message=FALSE, warning=FALSE, echo=FALSE, fig.height = 8, fig.width = 8}
p <- ggplot(data = to_plot |> filter(method == "seurat_de"),
             mapping = aes(y = p_value, col = pair_type)) +
   stat_qq_points(ymin = 1e-9, size = 0.8) +
   facet_wrap(~dataset_rename_w_pairs, scales = "free", ncol = 3, labeller = label_wrap_gen(35)) +
   geom_abline(col = "darkred") +
   stat_qq_band() +
   theme_bw() +
   scale_x_continuous(trans = revlog_trans(10)) +
   scale_y_continuous(trans = revlog_trans(10)) +
   labs(x = "Expected quantile", y = "Observed quantile") +
   theme(legend.position = "bottom",
         panel.grid.major = element_blank(),
         panel.grid.minor = element_blank(),
         panel.background = element_blank()) +
  scale_color_manual(values = c("chartreuse4", "dodgerblue3")) +
  labs(color = "Pair subset")
p
```

We do see a difference here. On the Frangieh datasets the QC'ed pairs demonstrate much better calibration than the randomly downsampled pairs. Finally, we plot the results for SCEPTRE.

```{r, fig.align = "center", message=FALSE, warning=FALSE, echo=FALSE, fig.height = 8, fig.width = 8}
p <- ggplot(data = to_plot |> filter(method == "sceptre"),
             mapping = aes(y = p_value, col = pair_type)) +
   stat_qq_points(ymin = 1e-9, size = 0.8) +
   facet_wrap(~dataset_rename_w_pairs, scales = "free", ncol = 3, labeller = label_wrap_gen(35)) +
   geom_abline(col = "darkred") +
   stat_qq_band() +
   theme_bw() +
   scale_x_continuous(trans = revlog_trans(10)) +
   scale_y_continuous(trans = revlog_trans(10)) +
   labs(x = "Expected quantile", y = "Observed quantile") +
   theme(legend.position = "bottom",
         panel.grid.major = element_blank(),
         panel.grid.minor = element_blank(),
         panel.background = element_blank()) +
  scale_color_manual(values = c("chartreuse4", "dodgerblue3")) +
  labs(color = "Pair subset")
p
```

The SCEPTRE results are similar to the Seurat DE results: QC leads to marked improvement on the Frangieh datasets but does not make much of a difference at all on the Schraivogel and Papalexi datasets. 

Next, we study the extent to which applying QC changes the composition of the pairs analyzed across datasets. I plot the percent decrease in the number of pairs analyzed per dataset after applying QC. The datasets that *do* use targeted sequencing (i.e., Schraivogel enhancer screen and Schraivogel TAP-seq) exhibit a small percentage decrease, whereas those that *do not* use targeted sequencing (the Frangieh and Papalexi datasets, the Schraivogel perturb-seq dataset) exhibit a large percentage decrease. The composition of pairs analyzed after applying QC is substantially different on the Frangieh datasets, the Papalexi dataset, and the Schraivogel perturb-seq dataset but not on the Schraivogel enhancer screen or TAP-seq datasets.

```{r, message=FALSE, warning=FALSE, echo=FALSE, fig.height = 5, fig.width = 4, fig.align = "center"}
n_pairs_per_dataset <- ntc_effective_samp_size |>
  filter(dataset %in% datasets_to_keep) |>
  group_by(dataset) |>
  summarize(count = n())

n_pairs_per_dataset_after_qc <- ntc_effective_samp_size |>
  filter(dataset %in% datasets_to_keep,
         effective_samp_size >= qc_thresh) |>
  group_by(dataset) |>
  summarize(count = n())

percent_decrease <- 100 * (n_pairs_per_dataset$count - n_pairs_per_dataset_after_qc$count)/n_pairs_per_dataset$count
df <- data.frame(dataset = n_pairs_per_dataset_after_qc$dataset,
                 percent_decrease = percent_decrease)

p <- ggplot(data = df, mapping = aes(x = dataset, y = percent_decrease)) + geom_bar(stat="identity") +
  ylab("Percent decrease after QC") +
  xlab("Dataset") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) 
p
```

# Is there latent confounding in the Schraivogel TAP-seq and perturb-seq datasets?

The Schraivogel perturb-seq and TAP-seq datsets exhibit miscalibration across methods. The miscalibration is a bit harder to detect on the TAP-seq data, as the TAP-seq dataset contains many fewer pairs than the perturb-seq dataset. However, if we downsample the perturb-seq dataset so that the perturb-seq dataset contains the same number of pairs as the TAP-seq dataset (2130), then the perturb-seq and TAP-seq datasets look similar. In fact, the perturb-seq dataset is better calibrated than the TAP-seq dataset.

```{r, message=FALSE, warning=FALSE, echo=FALSE, fig.height = 4, fig.width = 5, fig.align = "center"}
# determine n tapseq
set.seed(3)
n_tap_seq <- undercover_res |>
  filter(dataset == "schraivogel_ground_truth_tapseq_gene",
         method == "fisher_exact") |>
  nrow()

# downsample perturb-seq accordingly
undercover_res_perturbseq_downsample <- undercover_res |>
  filter(dataset == "schraivogel_ground_truth_perturbseq_gene",
         method == "fisher_exact") |>
  sample_n(size = n_tap_seq)

to_plot <- rbind(undercover_res |>
  filter(dataset == "schraivogel_ground_truth_tapseq_gene",
         method == "fisher_exact"),
  undercover_res_perturbseq_downsample)

p <- ggplot(data = to_plot, mapping = aes(y = p_value, col = dataset)) +
  stat_qq_points(ymin = 1e-9, size = 0.8) +
  geom_abline(col = "darkred") +
  stat_qq_band() +
  theme_bw() +
  scale_x_continuous(trans = revlog_trans(10)) +
  scale_y_continuous(trans = revlog_trans(10)) +
  labs(x = "Expected quantile", y = "Observed quantile") +
  theme(legend.position = "bottom",
        panel.grid.major = element_blank(),
         panel.grid.minor = element_blank(),
         panel.background = element_blank()) +
  scale_color_manual(values = c("chartreuse4", "dodgerblue3")) +
  labs(color = "Pair subset")
p
```

What, then, is going wrong with the TAP-seq and perturb-seq datasets? I hypothesized that there might be some amount of "latent confounding" causing miscalibration. To investigate this hypothesis, I ran a PCA on the (normalized) TAP-seq and perturb-seq gene expression datasets. The top PC of the TAP-seq dataset explained about 25% of the variance in the data; meanwhile, the top two PCs of the perturb-seq dataset explained about 30% of the variance. I added the top PC (respectively, top two PCs) of the TAP-seq (respectively, perturb-seq) dataset to the covariate matrix and reran SCPETRE and negative binomial regression. First, I present the results for SCEPTRE:

```{r, fig.align = "center", message=FALSE, warning=FALSE, echo=FALSE, fig.height = 4, fig.width = 8}
to_plot <- rbind(
  undercover_res |>
  filter(method %in% c("nb_regression", "sceptre"),
         dataset %in% c("schraivogel_ground_truth_tapseq_gene",
                        "schraivogel_ground_truth_perturbseq_gene")) |>
    mutate(with_top_pc = FALSE),
sceptre_nb_pca_res |>
  mutate(with_top_pc = TRUE))

p1 <- to_plot |>
  filter(method == "sceptre") |>
  ggplot(mapping = aes(y = p_value, col = with_top_pc)) +
  stat_qq_points(ymin = 1e-8, size = 0.8) +
  stat_qq_band() +
  theme_bw() +
  scale_x_continuous(trans = revlog_trans(10)) +
  scale_y_continuous(trans = revlog_trans(10)) +
  facet_wrap(dataset_rename_w_pairs~., scales = "free", labeller = label_wrap_gen(35)) +
  labs(col = "Top PC(s) included") + 
  geom_abline(col = "darkred") +
  theme(legend.position = "bottom")
p1
```

Second, I present the results for negative binomial regression:

```{r, fig.align = "center", message=FALSE, warning=FALSE, echo=FALSE, fig.height = 4, fig.width = 8}
p2 <- to_plot |>
  filter(method == "nb_regression") |>
  ggplot(mapping = aes(y = p_value, col = with_top_pc)) +
  stat_qq_points(ymin = 1e-8, size = 0.8) +
  stat_qq_band() +
  theme_bw() +
  scale_x_continuous(trans = revlog_trans(10)) +
  scale_y_continuous(trans = revlog_trans(10)) +
  facet_wrap(dataset_rename_w_pairs~., scales = "free", labeller = label_wrap_gen(35)) +
  labs(col = "Top PC(s) included") +
  geom_abline(col = "darkred") +
  theme(legend.position = "bottom")
p2
```

Including the top PC(s) does seem to improve the calibration of negative binomial regression slightly; by contrast, the calibration of SCEPTRE does not appear to change. Overall, the direction of searching for latent confounding via unsupervised learning does not appear to be super promising, but these results are worth keeping in mind.

# Examining the exact SCEPTRE p-values

I next examine the "exact" (as opposed to skew-normal) SCEPTRE p-values. I applied the "exact" version of SCEPTRE on all datasets, going out to B=100,000 permutations to obtain accurate p-values. I plot the exact SCEPTRE p-values alongside the skew-normal SCEPTRE p-values and the Fisher exact p-values. I facet along two axes: (i) dataset, and (ii) skew-normal fit quality ("good" or "bad"). I also plot the entire set of p-values (irrespective of skew-normal fit quality) alongside the pairs with "good" and "bad" skew-normal fits as a separate facet. I did not analyze the full set of pairs; I instead downsampled to save compute. Therefore, we are not looking as far out into the tail of the distribution as we typically have.

```{r, fig.align = "center", message=FALSE, warning=FALSE, echo=FALSE, fig.height = 20, fig.width = 8}
# define the categories
ks_stat_categories <- list("Good KS Fit" = c(0, 1e-2),
                           "Bad KS Fit" = c(1e-2, 1))

# We see the biggest improvement in calibration due to QC on the Frangieh data. Are pairs with "good" fits better calibrated than pairs with "non-good" fits?

# add fisher exact res to sceptre result
x <- left_join(x = sceptre_debug,
               y = fisher_exact_res |> rename(fisher_p_value = p_value),
               by = c("undercover_grna", "dataset", "response_id"))

to_plot <- x |>
  mutate(ks_fit = cut(x = ks_stat,
                      breaks = unique(unlist(ks_stat_categories)),
                      labels = names(ks_stat_categories))) |>
  select(p_value_sn = p_value,
         p_value_emp,
         fisher_p_value,
         dataset,
         ks_fit) |>
  pivot_longer(cols = c("p_value_sn", "p_value_emp", "fisher_p_value"),
               names_to = "method") |>
  mutate(method = factor(method,
                         levels = c("p_value_sn", "p_value_emp", "fisher_p_value"),
                         labels = c("Sceptre (SN)", "Sceptre (Emp)", "Fisher exact"))) |>
  update_dataset_names(TRUE) |>
  filter(dataset %in% datasets_to_keep)
to_plot <- rbind(to_plot, to_plot |> mutate(ks_fit = "All pairs"))

p <- to_plot |>
  ggplot(mapping = aes(y = value, col = Method)) +
  stat_qq_points(ymin = 1e-7, size = 0.6) +
  facet_grid(dataset_rename_w_pairs ~ ks_fit, scales = "free", labeller = label_wrap_gen(35)) +
  stat_qq_band() +
  theme_bw() +
  scale_x_continuous(trans = revlog_trans(10)) +
  scale_y_continuous(trans = revlog_trans(10)) +
  xlab("Expected p-value") +
  ylab("Observed p-value") +
  geom_abline(col = "darkred") +
  theme(legend.position = "bottom")
p
```

Note the following:

- The SCEPTRE exact p-values are cut off at 1e-5.
- We see miscalibration across methods on the Schraivogel TAP-seq and perturb-seq datasets.

# Conclusion: what is going on with the data?

We perhaps had hoped at the outset of the project that the same statistical problems would plague all the datasets. This, however, does not seem to be the case: each dataset poses its own unique set of analysis challenges. I do my best to summarize the dataset-specific analysis challenges below.

**Frangieh**: The Frangieh data are straightforward. There is no confounding. SCEPTRE and Seurat DE exhibit inflation when the effective sample size is small, as the asymptotic approximations that these methods make to the null distribution of the test statistic are not very good. As we apply more stringent QC (and thus as the effective sample size grows), asymptotics kick in and SCEPTRE and Seurat DE exhibit better calibration. The Fisher exact test --- a non-asymptotic test of independence --- is well-calibrated in all settings.

**Papalexi**: The Papalexi data are less transparent than the Frangieh data, but I believe that we are close to unraveling the mystery. QC does *not* make the problem easier, suggesting that something other than sparsity is causing problems. Also, as far as calibration quality goes, Fisher exact $\geq$ SCEPTRE > Seurat DE, with Fisher exact being slightly better than SCEPTRE. An important question to answer is thus as follows: why is Seurat DE (in contrast to SCEPTRE and Fisher exact) not well-calibrated on the Papalexi data?

**Schraivogel**: The Schraivogel data are the most mysterious of all. Essentially none of the methods (except for perhaps MIMOSCA) are calibrated on the TAP-seq or perturb-seq data. I have explored various possible causes of miscalibration, including latent confounding and NTC heterogeneity; neither was conclusive. Two further avenues for investigation are as follows. First, why is MIMOSCA calibrated on perturb-seq and TAP-seq (in the tail)? What is MIMOSCA doing differently than (for example) the exact version of SCEPTRE? Second, is it possible that some cells received both a non-targeting gRNA and a targeting gRNA, thereby causing some signal to creep into the negative control data? Third, what is the direction of the effect on the negative control data?
