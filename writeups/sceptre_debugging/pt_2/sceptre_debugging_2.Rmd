---
title: 'Statistical debugging of `sceptre` part 2'
author: "Tim"
date: "2022-09-06"
output: html_document
---

```{r, message = FALSE, echo = FALSE}
# load packages
library(tidyverse)
library(katlabutils)
result_dir <- paste0(.get_config_path("LOCAL_SCEPTRE2_DATA_DIR"), "results/")

# load the auxilliary functions
funct_script <- paste0(.get_config_path("LOCAL_CODE_DIR"), "sceptre2-manuscript/writeups/undercover_results_eda/analyze_undercover_results_plot_functs.R")
source(funct_script)

# 1. Fisher exact test negative control results
fisher_exact_res <- readRDS(paste0(result_dir, "undercover_grna_analysis/fisher_exact_1.rds"))
# 2. Positive control results
pc_res <- readRDS(paste0(result_dir, "positive_control_analysis/pc_results.rds"))
# 3. sample size information df
sample_size_df <- readRDS(paste0(result_dir, "dataset_sample_sizes/n_nonzero_cells_per_grna.rds")) |>
  dplyr::mutate(dataset = dataset_concat,
                dataset_concat = NULL, paper = NULL, modality = NULL) |>
  dplyr::rename(grna_group = target, response_id = feature_id)
# 4. undercover results (excluding Fisher) for group size = 1
undercover_res <- readRDS(paste0(result_dir, "undercover_grna_analysis/undercover_result_grp_size_1.rds")) |>
  dplyr::select(-clock_time, -max_ram) |>
  dplyr::filter(method %in% c("seurat_de", "liscovitch_method", "schraivogel_method", "weissman_method", "mimosca"))
# 5. undercover result sceptre (rbind with above undercover results)
undercover_res_sceptre <- readRDS(paste0(result_dir, "undercover_grna_analysis/undercover_result_grp_size_1_sceptre.rds")) |>
  dplyr::select(-clock_time, -max_ram)
undercover_res <- rbind(undercover_res, undercover_res_sceptre)
# 6. singleton positive control results
pc_res_singleton <- readRDS(paste0(result_dir, "positive_control_analysis/pc_results_singleton.rds")) |> rename("grna_id" = "grna_group")
# 7. undercover group size half (on Schraivogel perturb-seq data) results
undercover_res_grp_size_half <- readRDS(paste0(result_dir, "undercover_grna_analysis/undercover_result_grp_size_half_schraivogel_perturb_seq.rds"))
```

```{r, message = FALSE, echo = FALSE}
# Combine the results of the Schraivogel datasets
replace_slash_w_underscore <- function(df) {
  df |> dplyr::mutate(dataset = gsub(pattern = "/",
                                replacement = "_",
                                fixed = TRUE,
                                x = dataset))
}

fisher_exact_res_plot <- fisher_exact_res |>
  replace_slash_w_underscore() |>
  combine_schraivogel_enhancer_screens() |>
  update_dataset_names(TRUE)

sample_size_df <-
  sample_size_df |>
  replace_slash_w_underscore() |>
  combine_schraivogel_enhancer_screens()

pc_res <- pc_res |>
  replace_slash_w_underscore() |>
  combine_schraivogel_enhancer_screens() |>
  update_dataset_names()

pc_res_singleton <- pc_res_singleton |>
  replace_slash_w_underscore() |>
  combine_schraivogel_enhancer_screens() |>
  update_dataset_names()

undercover_res <- undercover_res |>
  replace_slash_w_underscore() |>
  combine_schraivogel_enhancer_screens() |>
  update_dataset_names()

undercover_res_grp_size_half <- undercover_res_grp_size_half |>
  replace_slash_w_underscore() |>
  update_dataset_names()
```

We seek to answer four outstanding questions in this writeup:

1.  Is the Fisher exact test calibrated on the negative control data? If so, can we draw conclusions about why the other methods might be miscalibrated?
2.  Why does the Fisher exact test yield a large fraction of p-values equal to exactly 1?
3.  Why are the methods poorly calibrated on the Schraivogel perturb-seq data?
4.  What QC threshold should we be using? If we adopt a more stringent QC threshold, do the methods remain miscalibrated?

# Question 1: Is the Fisher exact test calibrated on the negative control data?

I applied Fisher's exact test to the negative control data within the framework of the undercover pipeline (using group size = 1). Below, I plot the resulting *p*-values on a negative log 10 transformed scale.

```{r, echo=FALSE, fig.height = 7, fig.width = 8, fig.align = "center", cache=TRUE}
fisher_exact_res <- update_dataset_names(undercover_res = fisher_exact_res, TRUE)

p <- ggplot(data = fisher_exact_res_plot,
             mapping = aes(y = p_value)) +
   stat_qq_points(ymin = 1e-10, size = 0.8) +
   facet_wrap(~dataset_rename_w_pairs, scales = "free", ncol = 3, labeller = label_wrap_gen(35)) +
   geom_abline(col = "darkred") +
   stat_qq_band() +
   theme_bw() +
   scale_x_continuous(trans = revlog_trans(10)) +
   scale_y_continuous(trans = revlog_trans(10)) +
   labs(x = "Expected quantile", y = "Observed quantile") +
   theme(legend.position = "bottom",
         panel.grid.major = element_blank(),
         panel.grid.minor = element_blank(),
         panel.background = element_blank())
p
```

Amazingly, the *p*-values are calibrated far into the tail of the distribution on all of the datasets (with the exception of the Papalexi protein data and the Schraivogel perturb-seq data; more on those below). Hence, confounding likely is negligible (because if confounding were non-negligible, then the Fisher exact test, which is a test of *marginal* rather than *conditional* independence, would be miscalibrated). Seurat DE and SCEPTRE (not depicted) also are nonparametric tests of marginal independence. Both of these methods are miscalibrated on the negative control data. The miscalibration of Seurat DE and SCEPTRE likely is not the result of confounding; rather, the assumptions that Seurat DE and SCEPTRE make about the null distribution of the test statistic (i.e., that the test statistic is Gaussian distributed or skew-normal distributed) likely are wrong for some fraction of the pairs.

Next, I plot the negative control *p*-values on an untransformed scale.

```{r, echo=FALSE, fig.height = 7, fig.width = 8, fig.align = "center", cache=TRUE}
p <- ggplot(data = fisher_exact_res_plot,
             mapping = aes(y = p_value)) +
   stat_qq_points(ymin = 1e-10, size = 0.8) +
   facet_wrap(~dataset_rename_w_pairs, scales = "free", ncol = 3, labeller = label_wrap_gen(35)) +
   geom_abline(col = "darkred") +
   stat_qq_band() +
   theme_bw() +
   scale_x_reverse() +
   scale_y_reverse() +
   labs(x = "Expected quantile", y = "Observed quantile") +
   theme(legend.position = "bottom",
         panel.grid.major = element_blank(),
         panel.grid.minor = element_blank(),
         panel.background = element_blank())
p
```

A nontrivial fraction of the p-values is exactly equal to one, causing the bulk of the distribution to be conservative. I investigate reasons for this in the next section.

Finally, I applied Fisher's exact test to the (grouped) positive control data. The number of discoveries that the Fisher's exact test made after a Bonferoni correction is plotted below (the results of the other methods are plotted as well for reference). Clearly, Fisher's exact test quite capable of making discoveries. However, Fisher's exact test does appear to be slightly less powerful than the other methods (aside from MIMOSCA); this might be a result of the fact that Fisher's exact test is the only calibrated method among those plotted.

```{r, echo=FALSE, fig.height = 7, fig.width = 8, fig.align = "center", warning=FALSE, message=FALSE}
n_bonf_reject <- compute_n_bonf_rejected(pc_res)
p <- make_n_rejected_pairs_plot(n_rejected_df = n_bonf_reject,
                                y_max = NULL,
                                scales = "free",
                                log_trans = FALSE)
p
```

Some questions remain regarding the application of Fisher's exact test to single-cell CRISPR screen data. Nonetheless, if I had to advise a biologist tomorrow about what method she should use to analyze her single-cell CRISPR screen data, I would recommend Fisher's exact test.

# Question 2: Why does the Fisher exact test yield a large fraction of p-values exactly equal to one?

The Fisher exact test yields a *p*-value exactly equal to one for a large fraction of the negative control pairs. This likely is due to the fact that, for highly expressed features (e.g., protein data), the contingency table is extremely unbalanced. We might ameliorate this problem by dichotomizing the gene expression counts at their median rather than at zero.

# Question 3: Why are the methods miscalibrated on the Schraivogel perturb-seq data?

All methods (with the exception of MIMOSCA) are miscalibrated on the Schraivogel perturb-seq data. Why is this the case?

```{r, fig.align = "center", message=FALSE, warning=FALSE, echo=FALSE}
perturb_seq_to_plot <- rbind(undercover_res,
                             fisher_exact_res_plot) |>
                               filter(dataset == "schraivogel_ground_truth_perturbseq_gene")

p1 <- ggplot(data = perturb_seq_to_plot,
             mapping = aes(y = p_value, col = Method)) +
   stat_qq_points(ymin = 1e-10, size = 0.8) +
   facet_wrap(~dataset_rename_w_pairs, scales = "free", ncol = 3, labeller = label_wrap_gen(35)) +
   geom_abline(col = "darkred") +
   stat_qq_band() +
   theme_bw() +
   scale_x_continuous(trans = revlog_trans(10)) +
   scale_y_continuous(trans = revlog_trans(10)) +
   labs(x = "Expected quantile", y = "Observed quantile") +
   theme(legend.position = "bottom",
         panel.grid.major = element_blank(),
         panel.grid.minor = element_blank(),
         panel.background = element_blank())
p1
```

One possible explanation for the miscalibration that we observe is confounding. Gene carried out an analysis to assess the extent to which the perturbation indicators are confounded by the technical factors across datasets. Gene did not find any evidence of confounding on the Schraivogel perturb-seq data. Thus, confounding likely is not the culprit for the problems that we see here (unless, of course, there is unmeasured confounding).

I hypothesized that variability among the NTCs might be responsible for the miscalibration that we observe. If some NTCs actually have an effect (i.e., if some NTCs are not true NTCs), and if others have no effect, then the undercover *p*-values contain some amount of signal. To investigate this hypothesis, I carried out an undercover analysis in which I randomly partitioned the NTCs into two equally sized groups of "control" and "undercover" gRNAs, "averaging out" and thus mitigating the effect of any non-true NTC(s). (In other words, I ran the undercover gRNA pipeline, setting the group size equal to half the total number of NTCs.)

```{r, fig.align = "center", message=FALSE, warning=FALSE, echo=FALSE}
p1 <- ggplot(data = undercover_res_grp_size_half,
             mapping = aes(y = p_value, col = Method)) +
   stat_qq_points(ymin = 1e-10, size = 0.8) +
   facet_wrap(~dataset_rename_w_pairs, scales = "free", ncol = 3, labeller = label_wrap_gen(35)) +
   geom_abline(col = "darkred") +
   stat_qq_band() +
   theme_bw() +
   scale_x_continuous(trans = revlog_trans(10)) +
   scale_y_continuous(trans = revlog_trans(10)) +
   labs(x = "Expected quantile", y = "Observed quantile") +
   theme(legend.position = "bottom",
         panel.grid.major = element_blank(),
         panel.grid.minor = element_blank(),
         panel.background = element_blank())
p1
```

The calibration of the *p-*values improves considerably. However, the methods still are miscalibrated. I suspect that variability among the NTCs is at least partially responsible, but the full story still is not clear.

# Question 4: what is a meaningful QC threshold to use?

We must determine a reasonable QC threshold to use in our analysis. If we choose the QC threshold too liberally (i.e., if we allow too many pairs to pass through the QC filter), then we risk putting ourselves in an "uninteresting" region of the problem space in which we are under-powered to make discoveries for pairs with low effective sample sizes. This not only wastes compute but also causes a reduction in power at the multiple testing correction step. By contrast, if we choose our QC threshold too stringently (i.e., if we allow too few pairs to pass through the QC filter), then we risk throwing out biologically interesting pairs that we might have otherwise rejected. An additional complication is that the "effective sample size" of a given pair (i.e., the number of cells with nonzero gene expression) is a function of both the gene and the gRNA. Thus, QC in single-cell CRISPR screen analysis must operate at the level of the gene-gRNA pair (instead of at the level of the gene and the gRNA separately).

A reasonable strategy for selecting the QC threshold is as follows: locate the minimum effective sample size such that a well-calibrated method (e.g., Fisher's exact test) consistently rejects positive control pairs whose effective sample size is (approximately) equal to that minimizer. To determine this threshold empirically, I applied Fisher's exact test, SCEPTRE, the Weissman Method, and Seurat DE to the singleton (i.e., ungrouped) positive control data. (I did not apply MIMOSCA and Schraivogel method, as these methods are computationally expensive. Furthermore, I did not apply Liscovitch method, as Liscovitch method is badly miscalibrated and thus uninformative.) First, I plot the number of rejections that each method makes on each dataset.

```{r, message=FALSE, warning=FALSE, echo=FALSE}
# Compute the (true and effective) sample size for each row 
pc_res_singleton_w_samp_size <-
  dplyr::left_join(pc_res_singleton,
                   select(sample_size_df, -grna_group),
                   by = c("response_id", "grna_id", "dataset")) |>
  dplyr::rename("n_nonzero_cells_trt" = "n_nonzero_cells",
                "n_cells_trt" = "n_cells")

response_ids <- as.character(unique(pc_res_singleton_w_samp_size$response_id))
# Next, true and effective sample size of the negative control cells for each response id
x <- sample_size_df |>
  filter(grna_group == "non-targeting",
         response_id %in% response_ids) |>
  group_by(response_id, dataset) |>
  summarize(n_nonzero_cells_ntc = sum(n_nonzero_cells),
            n_cells_ntc = sum(n_cells)) |>
  ungroup()
pc_res_singleton_w_samp_size_2 <- left_join(pc_res_singleton_w_samp_size, x,
                                          by = c("response_id", "dataset")) |> 
  mutate(effective_sample_size = n_nonzero_cells_trt + n_nonzero_cells_ntc) |>
  filter(method != "liscovitch_method")
```

```{r, fig.height = 7, fig.width = 8, fig.align = "center", message=FALSE, warning=FALSE, echo=FALSE}
n_bonf_reject <- compute_n_bonf_rejected(pc_res_singleton_w_samp_size_2)
p <- make_n_rejected_pairs_plot(n_rejected_df = n_bonf_reject,
                                y_max = NULL,
                                scales = "free",
                                log_trans = FALSE)
p
```

Seurat DE and SCEPTRE appear to be the most powerful methods in general, with Seurat DE having a slight advantage over SCEPTRE. Next, I plot a histogram of the "effective sample size" of each pair, faceted by dataset.

```{r, fig.height = 7, fig.width = 8, fig.align = "center", message=FALSE, warning=FALSE, echo=FALSE}
pc_res_singleton_w_samp_size_2 |>
  ggplot(mapping = aes(x = effective_sample_size)) +
  geom_histogram(bins = 20) + facet_wrap(dataset_rename_w_pairs~.,
                                         scales = "free",
                                         labeller = label_wrap_gen(35)) +
  theme_bw() +
  ylab("N pairs") +
  xlab("Effective sample size")
```

Effective sample sizes for the (singleton) positive control pairs range from zero up to several thousand in most cases (and up to 10,000+ on the Schraivogel ground truth perturb-seq data).

Next, I plot the Fisher exact *p*-value vs. the effective sample size of each pair on each of the positive control datasets. Pairs with a *p*-value of above 0.001 are colored in red and those below 0.001 are colored in blue. (The number 0.001 was chosen somewhat arbitrarily; basically, 0.001 is a proxy for the threshold that a *p*-value might have to surpass for the pair to be rejected.) The vertical black line indicates the pair with the smallest effective sample size such that its *p*-value is less than 0.001. Across datasets, we see that the minimum effective sample size required to reject a pair at the 0.001 level is about 500.

```{r, message=FALSE, warning=FALSE, echo=FALSE, fig.height = 7, fig.width = 9, fig.align = "center"}
to_plot <- pc_res_singleton_w_samp_size_2 |>
  mutate(p_lt_001 = p_value <= 0.001) |>
  mutate(p_lt_001_f = factor(p_lt_001, levels = c(FALSE, TRUE), labels = c("no", "yes"))) |>
  group_by(method, dataset) |>
  mutate(qc_thresh = min(effective_sample_size[p_lt_001])) |>
  arrange(p_lt_001) |>
  ungroup()

to_plot |>
  filter(method == "fisher_exact") |>
  ggplot(mapping = aes(x = effective_sample_size,
                       y = p_value,
                       col = p_lt_001)) +
  facet_wrap(dataset_rename_w_pairs ~ ., scales = "free", labeller = label_wrap_gen(35)) +
  geom_point(size = 0.85) + xlab("Effective sample size") + ylab("p-value") + xlim(c(0, NA)) + theme_bw() +
  scale_color_manual(values = c("darkred", "darkblue")) + labs(color = "p < 0.001") + geom_vline(aes(xintercept = qc_thresh))
```

We the same plot for Seurat DE. The results are similar to those of the Fisher exact test.

```{r, message=FALSE, warning=FALSE, echo=FALSE, fig.height = 7, fig.width = 9, fig.align = "center"}
to_plot |>
  filter(method == "seurat_de") |>
  ggplot(mapping = aes(x = effective_sample_size,
                       y = p_value,
                       col = p_lt_001)) +
  facet_wrap(dataset_rename_w_pairs ~ ., scales = "free", labeller = label_wrap_gen(35)) +
  geom_point(size = 0.85) + xlab("Effective sample size") + ylab("p-value") + xlim(c(0, NA)) + theme_bw() +
  scale_color_manual(values = c("darkred", "darkblue")) + labs(color = "p < 0.001") + geom_vline(aes(xintercept = qc_thresh))
```

The apparent minimum effective sample size required to reject at a 0.001 level (i.e., \$\\approx\$ 250 - 750) is surprisingly large.

# The impact stringent QC on calibration

As we apply increasingly stringent QC, the problem becomes easier, and thus calibration should improve. We test that hypothesis here. First, we plot the results that we obtain using the default QC (i.e., filtering for genes that are expressed in > 0.005 of cells).

```{r, warning=FALSE, echo=FALSE, message = FALSE, fig.height = 7, fig.width = 9, fig.align = "center"}
ntc_effective_samp_size <- sample_size_df |>
  filter(grna_group == "non-targeting") |>
  group_by(response_id, dataset) |>
  summarize(effective_samp_size = sum(n_nonzero_cells))
# mean(ntc_effective_samp_size$effective_samp_size >= 250)

undercover_res_w_samp_size <-
  rbind(undercover_res |> filter(!(dataset %in%
                                     c("liscovitch_experiment_big_chromatin",
                                       "liscovitch_experiment_small_chromatin"))),
        fisher_exact_res_plot) |>
  left_join(ntc_effective_samp_size, by = c("response_id", "dataset"))

make_trans_qq_plot(undercover_res_w_samp_size)
```

Next, we remove all pairs with an effective sample size of less than 250. This amounts to removing 30% of pairs. The calibration improves, especially on the fairly sparse Frangieh data.

```{r, warning=FALSE, echo=FALSE, fig.height = 7, fig.width = 9, fig.align = "center"}
undercover_res_w_samp_size |>
  filter(effective_samp_size >= 250) |>
  select(-Method, -dataset_rename_w_pairs, -effective_samp_size, -n_pairs) |>
  update_dataset_names(TRUE) |>
  make_trans_qq_plot()
```

Next, we restrict our attention to pairs that have an effective sample size exceeding 750. This amounts to removing 50% of (the original) pairs. Calibration improves further.

```{r, warning=FALSE, echo=FALSE, fig.height = 7, fig.width = 9, fig.align = "center"}
# mean(ntc_effective_samp_size$effective_samp_size >= 750)

undercover_res_w_samp_size |>
  filter(effective_samp_size >= 750) |>
  select(-Method, -dataset_rename_w_pairs, -effective_samp_size, -n_pairs) |>
  update_dataset_names(TRUE) |>
  make_trans_qq_plot()
```

For better or for worse, when we restrict our attention to more "meaningful" regions of the parameter space, Seurat DE becomes a formidable competitor!

# Next steps and open questions

-   One reason that the methods appear to be better calibrated when we apply more stringent QC is that we are analyzing fewer pairs and thus not looking as far out into the tail of the null distribution. We could look further out into the tail by increasing the group size in the undercover analysis (to, say, two) and iterating through a larger number of groups.

-   Might it be worthwhile to re-analyze the Perturb-seq data by running PCA on the expression matrix and including the top PCs as covariates? This could help to capture any unmeasured confounding.

-   We have roughly 9.5 weeks until December 15. Can we come up with a week-by-week plan to get a working manuscript finished by that deadline? Leaving 3 weeks to making figures and writing, this leaves about 6.5 weeks for finalizing results.
