---
title: 'Statistical debugging of `sceptre` part 2'
author: "Tim"
date: "2022-09-06"
output: html_document
---

```{r, message = FALSE, echo = FALSE}
# load packages
library(tidyverse)
library(katlabutils)
result_dir <- paste0(.get_config_path("LOCAL_SCEPTRE2_DATA_DIR"), "results/")

# load the auxilliary functions
funct_script <- paste0(.get_config_path("LOCAL_CODE_DIR"), "sceptre2-manuscript/writeups/undercover_results_eda/analyze_undercover_results_plot_functs.R")
source(funct_script)

# load results
# 1. Fisher exact test negative control results
# 2. Fisher exact test positive control results
# 3. effective sample size data frame (number of cells with nonzero expression per gRNA)
# 4. undercover result for group size = 1

# 1. 
fisher_exact_res <- readRDS(paste0(result_dir, "undercover_grna_analysis/fisher_exact_1.rds"))
# 2.
pc_res <- readRDS(paste0(result_dir, "positive_control_analysis/pc_results.rds"))
# 3.
sample_size_df <- readRDS(paste0(result_dir, "dataset_sample_sizes/n_nonzero_cells_per_grna.rds")) |>
  dplyr::mutate(dataset = dataset_concat,
                dataset_concat = NULL, paper = NULL, modality = NULL) |>
  dplyr::rename(grna_group = target, response_id = feature_id)
# 4.
undercover_res <- readRDS(paste0(result_dir, "undercover_grna_analysis/undercover_result_grp_size_1.rds")) |>
  dplyr::select(-clock_time, -max_ram) |>
  dplyr::filter(method %in% c("seurat_de", "liscovitch_method", "schraivogel_method", "weissman_method", "mimosca"))
# 5.
undercover_res_sceptre <- readRDS(paste0(result_dir, "undercover_grna_analysis/undercover_result_grp_size_1_sceptre.rds")) |>
  dplyr::select(-clock_time, -max_ram)
undercover_res <- rbind(undercover_res, undercover_res_sceptre)
```

```{r, message = FALSE, echo = FALSE}
# Combine the results of the Schraivogel datasets
replace_slash_w_underscore <- function(df) {
  df |> dplyr::mutate(dataset = gsub(pattern = "/",
                                replacement = "_",
                                fixed = TRUE,
                                x = dataset))
}

fisher_exact_res_plot <- fisher_exact_res |>
  replace_slash_w_underscore() |>
  combine_schraivogel_enhancer_screens() |>
  update_dataset_names(TRUE)

sample_size_df <-
  sample_size_df |>
  replace_slash_w_underscore() |>
  combine_schraivogel_enhancer_screens()

pc_res <- pc_res |>
  replace_slash_w_underscore() |>
  combine_schraivogel_enhancer_screens() |>
  update_dataset_names()

undercover_res <- undercover_res |>
  replace_slash_w_underscore() |>
  combine_schraivogel_enhancer_screens() |>
  update_dataset_names()
```

We seek to answer four outstanding questions in this writeup:

1.  Is the Fisher exact test calibrated on the negative control data? If so, can we draw conclusions about why the other methods might be miscalibrated?
2.  Why does the Fisher exact test yield a large fraction of p-values equal to exactly 1?
3.  Why are the methods poorly calibrated on the Schraivogel perturb-seq data?
4.  What QC threshold should we be using? If we adopt a more stringent QC threshold, do the methods remain miscalibrated?

# Question 1: Is the Fisher exact test calibrated on the negative control data?

I applied Fisher's exact test to the negative control data within the framework of the undercover pipeline (using group size = 1). Below, I plot the resulting *p*-values on a negative log 10 transformed scale.

```{r, echo=FALSE, fig.height = 7, fig.width = 8, fig.align = "center", cache=TRUE}
fisher_exact_res <- update_dataset_names(undercover_res = fisher_exact_res, TRUE)

p <- ggplot(data = fisher_exact_res_plot,
             mapping = aes(y = p_value)) +
   stat_qq_points(ymin = 1e-10, size = 0.8) +
   facet_wrap(~dataset_rename_w_pairs, scales = "free", ncol = 3, labeller = label_wrap_gen(35)) +
   geom_abline(col = "darkred") +
   stat_qq_band() +
   theme_bw() +
   scale_x_continuous(trans = revlog_trans(10)) +
   scale_y_continuous(trans = revlog_trans(10)) +
   labs(x = "Expected quantile", y = "Observed quantile") +
   theme(legend.position = "bottom",
         panel.grid.major = element_blank(),
         panel.grid.minor = element_blank(),
         panel.background = element_blank())
p
```

Amazingly, the *p*-values are calibrated far into the tail of the distribution on all of the datasets (with the exception of the Papalexi protein data and the Schraivogel perturb-seq data; more on those below). Hence, confounding likely is negligible (because if confounding were non-negligible, then the Fisher exact test, which is a test of *marginal* rather than *conditional* independence, would be miscalibrated). Seurat DE and SCEPTRE (not depicted) also are nonparametric tests of marginal independence. Both of these methods are miscalibrated on the negative control data. The miscalibration of Seurat DE and SCEPTRE likely is not the result of confounding; rather, the assumptions that Seurat DE and SCEPTRE make about the null distribution of the test statistic (i.e., that the test statistic is Gaussian distributed or skew-normal distributed) likely are wrong for some fraction of the pairs.

Next, I plot the negative control *p*-values on an untransformed scale.

```{r, echo=FALSE, fig.height = 7, fig.width = 8, fig.align = "center", cache=TRUE}
p <- ggplot(data = fisher_exact_res_plot,
             mapping = aes(y = p_value)) +
   stat_qq_points(ymin = 1e-10, size = 0.8) +
   facet_wrap(~dataset_rename_w_pairs, scales = "free", ncol = 3, labeller = label_wrap_gen(35)) +
   geom_abline(col = "darkred") +
   stat_qq_band() +
   theme_bw() +
   scale_x_reverse() +
   scale_y_reverse() +
   labs(x = "Expected quantile", y = "Observed quantile") +
   theme(legend.position = "bottom",
         panel.grid.major = element_blank(),
         panel.grid.minor = element_blank(),
         panel.background = element_blank())
p
```

A nontrivial fraction of the p-values is exactly equal to one, causing the bulk of the distribution to be conservative. I investigate reasons for this in the next section.

Finally, I applied Fisher's exact test to the (grouped) positive control data. The number of discoveries that the Fisher's exact test made after a Bonferoni correction is plotted below (the results of the other methods are plotted as well for reference). Clearly, Fisher's exact test quite capable of making discoveries. However, Fisher's exact test does appear to be slightly less powerful than the other methods (aside from MIMOSCA); this might be a result of the fact that Fisher's exact test is the only calibrated method among those plotted.

```{r, echo=FALSE, fig.height = 7, fig.width = 8, fig.align = "center", warning=FALSE, message=FALSE}
n_bonf_reject <- compute_n_bonf_rejected(pc_res)
p <- make_n_rejected_pairs_plot(n_rejected_df = n_bonf_reject,
                                y_max = NULL,
                                scales = "free",
                                log_trans = FALSE)
p
```

Some questions remain regarding the application of Fisher's exact test to single-cell CRISPR screen data (for example, how should the contingency table be constructed for highly expressed features)? Nonetheless, if I had to advise a biologist tomorrow about what method he should use to analyze his single-cell CRISPR screen data, I would recommend Fisher's exact test.

# Question 2: Why does the Fisher exact test yield a large fraction of p-values exactly equal to one?

The Fisher exact test yields a *p*-value exactly equal to one for

# Question 3: Why are the methods miscalibrated on the Schraivogel perturb-seq data?

All methods (with the exception of MIMOSCA) are miscalibrated on the Schraivogel perturb-seq data. Why is this the case?

```{r, fig.align = "center", message=FALSE, warning=FALSE, echo=FALSE}
perturb_seq_to_plot <- rbind(undercover_res,
                             fisher_exact_res_plot) |>
                               filter(dataset == "schraivogel_ground_truth_perturbseq_gene")

p <- ggplot(data = perturb_seq_to_plot,
             mapping = aes(y = p_value, col = Method)) +
   stat_qq_points(ymin = 1e-10, size = 0.8) +
   facet_wrap(~dataset_rename_w_pairs, scales = "free", ncol = 3, labeller = label_wrap_gen(35)) +
   geom_abline(col = "darkred") +
   stat_qq_band() +
   theme_bw() +
   scale_x_continuous(trans = revlog_trans(10)) +
   scale_y_continuous(trans = revlog_trans(10)) +
   labs(x = "Expected quantile", y = "Observed quantile") +
   theme(legend.position = "bottom",
         panel.grid.major = element_blank(),
         panel.grid.minor = element_blank(),
         panel.background = element_blank())
p
```

One possible explanation for the miscalibration that we observe is confounding. Gene carried out an analysis to assess the extent to which the perturbation indicators are confounded by the technical factors across datasets. Gene did not find any evidence of confounding on the Schraivogel perturb-seq data. Thus, confounding likely is not the culprit for the problems that we see here (unless, of course, there is unmeasured confounding, which is challenging to assess).

I hypothesized that variability among the NTCs might be responsible for the miscalibration that we observe. If some NTCs actually have an effect (i.e., if some NTCs are not true NTCs), and if others have no effect, then the undercover *p*-values would contain some amount of signal. To investigate this hypothesis, I carried out an undercover analysis in which I randomly partitioned the NTCs into two equally sized groups of "control" and "undercover" gRNAs. This procedure should help to "average out" and thus mitigate the effect of any non-true NTC(s).
