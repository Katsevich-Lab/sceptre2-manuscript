---
title: 'Debugging sceptre 5: Attempting to unravel the Schraivogel data'
author: "Tim"
date: "2022-10-24"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE, error=FALSE, warning=FALSE, echo=FALSE}
library(katlabutils)
library(tidyverse)
result_dir <- paste0(.get_config_path("LOCAL_SCEPTRE2_DATA_DIR"), "results/")

# load results
# 1. Fisher exact res
fisher_exact_res <- readRDS(paste0(result_dir, "undercover_grna_analysis/fisher_exact_1.rds"))

# 2. Fisher debug
fisher_debug <- readRDS(paste0(result_dir, "undercover_grna_analysis/fisher_debug.rds"))

# 3. MW-MIMOSCA
mw_mim <- readRDS(paste0(result_dir, "undercover_grna_analysis/mw_mimosca_1.rds"))

# 4. MW
mw_res <- readRDS(paste0(result_dir, "undercover_grna_analysis/undercover_result_grp_size_1.rds")) |>
  filter(method == "seurat_de") |>
  select(-clock_time, -max_ram)

# 5. MIMOSCA
mimosca_res <- readRDS(paste0(result_dir,
                              "undercover_grna_analysis/undercover_result_grp_size_1.rds")) |>
  select(-clock_time, -max_ram) |>
  filter(method == "mimosca")

# source functions
funct_script <- paste0(.get_config_path("LOCAL_CODE_DIR"), "sceptre2-manuscript/writeups/undercover_results_eda/analyze_undercover_results_plot_functs.R")
source(funct_script)
```

## Understanding the Schraivogel data

The Schraivogel TAP-seq and perturb-seq data are anomalous. Here, I seek to understand why the Schraivogel data are so different from the other data. Previously, I tried to ameliorate the miscalibration on the Schraivogel data by including the top PCs of the gene expression matrix as covariates; this did not resolve the problem. Furthermore, I tested the hypothesis that some NT gRNAs might not in fact be NT gRNAs (i.e., that they have an effect); this analysis was (at best) inconclusive. Here, I investigate two further questions.

- First, does changing the cell-to-gRNA assignment threshold (from 8 to, e.g., 5) improve the results? In other words, if we exlude cells that contain two or more gRNAs with 5+ (as opposed to 8+) UMI counts, does this improve calibration? The idea here is that cells containing an NT gRNA may also contain a targeting gRNA, thereby injecting some "signal" into the negative control data.

- Second, does changing the set of control cells from those that receive an NT gRNA to the compliment of those that receive a targeting gRNA (i.e., the "compliment set") improve the results?

```{r, message = FALSE, echo = FALSE}
replace_slash_w_underscore <- function(df) {
  df |> dplyr::mutate(dataset = gsub(pattern = "/",
                                replacement = "_",
                                fixed = TRUE,
                                x = dataset))
}

fisher_debug <- fisher_debug |>
  replace_slash_w_underscore() |>
  update_dataset_names()
mw_mim <- mw_mim |>
  replace_slash_w_underscore() |>
  update_dataset_names(TRUE)
mw_res <- mw_res |>
  filter(dataset %in% unique(mw_mim$dataset)) |>
  update_dataset_names(TRUE)
```

I answered these questions by applying Fisher's exact test to the TAP-seq and perturb-seq datasets. I used two different control groups: first, the set of cells that received an NT, and second, the compliment set. I also varied the threshold for assigning gRNAs to cells over the set $\{1, 3, 5, 7, 8\}$. The results are presented below.

```{r, message = FALSE, echo = FALSE, fig.height = 8, fig.width = 8, fig.align = "center"}
min_count <- fisher_debug |>
  group_by(grna_threshold, control_group) |>
  summarize(count = n()) |>
  pull(count) |>
  min()

p <- fisher_debug |>
  mutate(grna_threshold = factor(grna_threshold)) |>
  ggplot(mapping = aes(y = p_value, col = grna_threshold)) +
   stat_qq_points(ymin = 1e-9, size = 0.8) +
   facet_grid(dataset_rename_w_pairs ~ control_group, scales = "free",
              labeller = label_wrap_gen(35)) +
   geom_abline(col = "darkred") +
   stat_qq_band() +
   theme_bw() +
   scale_x_continuous(trans = revlog_trans(10)) +
   scale_y_continuous(trans = revlog_trans(10)) +
   labs(x = "Expected quantile", y = "Observed quantile") +
   theme(legend.position = "bottom",
         panel.grid.major = element_blank(),
         panel.grid.minor = element_blank(),
         panel.background = element_blank())
p
```

The Perturb-seq results are presented in the top row, and the TAP-seq datasets are presented in the bottom row. Meanwhile, the results based on using the compliment set as the control cells are presented in the first column, while those based on using NT cells as the control cells are presented in the second. Overall, choice of the control group (i.e., compliment set versus NT cells) does not seem to make much of a difference. Furthermore, the gRNA-to-cell assignment threshold likewise does not make much of a difference on the Perturb-seq data. On the TAP-seq data, by contrast, setting the gRNA threshold to 1 (a very strict threshold) does seem to improve calibration of the Fisher exact test.

## Builing the null distribution by sharing information across genes

Recall that MIMOSCA is decently calibrated (at least in the tail) on the Perturb-seq and TAP-seq data. One possible explanation for this is that MIMOSCA builds its null distribution by sharing information across genes. (Specifically, for a given gRNA, MIMOSCA fits elastic net regressions of all genes onto the gRNA; then, MIMOSCA permutes the gRNA assignments and fits additional elastic net regressions by regressing the genes onto the permuted gRNA assignments. MIMOSCA uses as its null distribution the elastic net coefficients obtained from the regressions of the genes onto the permuted gRNA assignments.) I evaluated a version of the Mann-Whitney test in which I generated the null distribution by sharing information across genes in a way that is similar to MIMOSCA. The results are plotted below. The horizontal black line indicates the minimum possible p-value (based on the number of permutations; B = 50).

```{r, message = FALSE, echo = FALSE, fig.height = 10, fig.width = 10, fig.align = "center"}
min_p_df <- mw_mim |>
  group_by(dataset_rename_w_pairs) |>
  summarize(n_genes = length(unique(response_id))) |>
  mutate(min_p = 2/(n_genes * 50))

to_plot <- rbind(mw_mim, mw_res)

to_plot |>
  ggplot(mapping = aes(y = p_value, col = Method)) +
  stat_qq_points(ymin = 1e-9, size = 0.8) +
  facet_wrap(dataset_rename_w_pairs ~ .,
             scales = "free",
             labeller = label_wrap_gen(35)) +
  scale_x_continuous(trans = revlog_trans(10)) +
  scale_y_continuous(trans = revlog_trans(10)) +
  theme(legend.position = "bottom",
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.background = element_blank()) +
  geom_abline(col = "darkred") +
  stat_qq_band() +
  geom_hline(mapping = aes(yintercept = min_p), data = min_p_df) +
  ylab("p-value")
```

Overall, the results look fairly good and are similar to those of the Fisher exact test. (The bulk calibration is slightly better while the tail calibration is slightly worse.) However, the strategy of building a null distribution by sharing information across genes does not ameliorate the calibration issues observed on the Schraivogel data. I plot the same results on an untransformed scale.

```{r, message = FALSE, echo = FALSE, fig.height = 10, fig.width = 10, fig.align = "center"}
to_plot |>
  ggplot(mapping = aes(y = p_value, col = Method)) +
  stat_qq_points(ymin = 1e-9, size = 0.8) +
  facet_wrap(dataset_rename_w_pairs ~ .,
             scales = "free",
             labeller = label_wrap_gen(35)) +
  theme(legend.position = "bottom",
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.background = element_blank()) +
  geom_abline(col = "darkred") +
  stat_qq_band() +
  geom_hline(mapping = aes(yintercept = min_p), data = min_p_df) +
  ylab("p-value")
```

The cause of miscalibration on the Perturb-seq data remains illusive.

Finally, I study the range of the p-values outputted by MIMOSCA.

```{r}
mimosca_res |>
  group_by(dataset) |>
  summarize(min_p_val = min(p_value),
            max_p_val = max(p_value))

mimosca_res |> nrow()
```

This is clear evidence of pathological behavior. For example, the maximum p-value that MIMOSCA outputted on the Frangieh IFN-gamma data is 0.044. Moreover, of the > 4 million hypotheses that we tested across 12 datasets, the maximum p-value that MIMOSCA outputted is 0.52. (This number, of course, should be about 1.) Given MIMOSCA's unusual behavior and apparent inability to output p-values that are greater than about 0.5, making extensive comparisons to MIMOSCA may not be worthwhile.
