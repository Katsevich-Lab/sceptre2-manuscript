---
title: 'Debuging `sceptre` 5: Understanding Papalexi and MIMOSCA'
author: "Tim"
date: "2022-10-15"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(katlabutils)
library(tidyverse)
result_dir <- paste0(.get_config_path("LOCAL_SCEPTRE2_DATA_DIR"), "results/")
datasets_to_study <- c("frangieh_co_culture_gene", "papalexi_eccite_screen_gene",
                        "frangieh_control_gene", "frangieh_ifn_gamma_gene")
# define the categories
ks_stat_categories_simple <- list(good = c(0, 1e-2),
                                  bad = c(1e-2, 1))
ks_stat_categories <- list(good = c(0, 1e-2),
                           adequate = c(1e-2, 5e-2),
                           subpar = c(5e-2, 0.1),
                           poor = c(0.1, 1))
# load results
# 1. sceptre debug
sceptre_debug <- readRDS(paste0(result_dir, "undercover_grna_analysis/sceptre_debug.rds")) |>
  mutate(p_gap = abs(p_value - p_value_emp),
         p_rat = p_value_emp/p_value,
         ks_fit_simple = cut(x = ks_stat,
                             breaks = unique(unlist(ks_stat_categories_simple)),
                             labels = names(ks_stat_categories_simple)),
         ks_fit = cut(x = ks_stat,
                      breaks = unique(unlist(ks_stat_categories)),
                      labels = names(ks_stat_categories)),
         dataset_slash = dataset)

# 2. undercover res sceptre
undercover_res <- readRDS(paste0(result_dir,
                                 "undercover_grna_analysis/undercover_result_grp_size_1_sceptre.rds")) |>
  dplyr::select(-clock_time, -max_ram) |>
  dplyr::filter(method == "sceptre",
                dataset %in% datasets_to_study)

# 3. undercover res seurat de
undercover_res_sde <- readRDS(paste0(result_dir,
                                     "undercover_grna_analysis/undercover_result_grp_size_1.rds")) |>
  dplyr::select(-clock_time, -max_ram) |>
  filter(method == "seurat_de",
         dataset %in% datasets_to_study)

# 4. sample size df
sample_size_df <- readRDS(paste0(result_dir,
                                 "dataset_sample_sizes/n_nonzero_cells_per_grna.rds")) |>
  dplyr::mutate(dataset = dataset_concat,
                dataset_concat = NULL, paper = NULL, modality = NULL) |>
  dplyr::rename(grna_group = target, response_id = feature_id)

# 4. MW debug
mw_debug <- readRDS(paste0(result_dir,
                           "undercover_grna_analysis/mw_debug.rds"))
# source functions
funct_script <- paste0(.get_config_path("LOCAL_CODE_DIR"), "sceptre2-manuscript/writeups/undercover_results_eda/analyze_undercover_results_plot_functs.R")
source(funct_script)
```

```{r, echo=FALSE}
replace_slash_w_underscore <- function(df) {
  df |> dplyr::mutate(dataset = gsub(pattern = "/",
                                replacement = "_",
                                fixed = TRUE,
                                x = dataset))
}

update_df <- function(df) {
  df |>
    replace_slash_w_underscore() |>
    combine_schraivogel_enhancer_screens() |>
    update_dataset_names(add_n_pairs = TRUE)
}

undercover_res <- undercover_res |>
  replace_slash_w_underscore() |>
  update_dataset_names(add_n_pairs = TRUE)
sceptre_debug <- update_df(sceptre_debug) |>
  filter(dataset %in% datasets_to_study)
sample_size_df <- sample_size_df |>
  replace_slash_w_underscore() |>
  combine_schraivogel_enhancer_screens()
mw_debug <- mw_debug |>
  replace_slash_w_underscore()
```

This is the fifth installment of the debugging SCEPTRE series. I aim to answer the following questions.

1. Why is SCEPTRE reasonably well-calibrated on the Papalexi data but not on the Frangieh data?
2. Why does applying QC improve the quality of SCEPTRE on the Papalexi data but not on the Frangieh data?
3. Why is Seurat DE less well calibrated than SCEPTRE on the Papalexi data?
4. Why is MIMOSCA the only calibrated method on the Schraivogel TAP-seq and Perturb-seq data?

# Part 1: SCEPTRE exhibits better calibration on the Papalexi data than the Frangieh data. Also, applying QC improves the calibration quality of SCEPTRE on the Frangieh data but not on the Papalexi data.

I begin by comparing the SCEPTRE results obtained on the Frangieh data to those obtained on the Papalexi data. I downsample the set of Frangieh pairs so that the Frangieh data contain the same number of pairs as the Papalexi data. SCEPTRE clearly exhibits better calibration on the Papalexi data than on the Frangieh data.

```{r, echo = FALSE, fig.align = "center", fig.height = 6, fig.width = 7, message=FALSE, warning=FALSE}
# first, downsample Frangieh pairs so that Frangieh and Papalexi are comparable
n_papalexi <- undercover_res |>
  filter(dataset == "papalexi_eccite_screen_gene") |>
  nrow()
undercover_res_samp <- 
  undercover_res |>
  group_by(dataset) |>
  sample_n(size = n_papalexi) |>
  update_dataset_names()

# next, just plot the results, faceting by dataset, to confirm that Frangieh is in fact worse than Papalexi
undercover_res_samp |>
  ggplot(mapping = aes(y = p_value)) +
   stat_qq_points(ymin = 1e-9, size = 0.8) +
   facet_wrap(~dataset_rename_w_pairs, scales = "free",
              ncol = 2,
              labeller = label_wrap_gen(35)) +
   geom_abline(col = "darkred") +
   stat_qq_band() +
   theme_bw() +
   scale_x_continuous(trans = revlog_trans(10)) +
   scale_y_continuous(trans = revlog_trans(10)) +
   labs(x = "Expected quantile", y = "Observed quantile") +
   theme(legend.position = "bottom",
         panel.grid.major = element_blank(),
         panel.grid.minor = element_blank(),
         panel.background = element_blank())
```
Next, for each dataset, I plot the pairs that pass QC alongside the pairs that do not. The comparison here is along two axes: first, across datasets, and second, within a given dataset, across pairs that passed QC versus pairs that did not. To enable meaningful comparisons along both axes, I performed downsampling so that each dataset-QC status (i.e., passed versus not passed) tuple would contain the same number of pairs. In other words, I performed downsampling so that each of the eight sets of pairs below (two for each of the four plots) would have the same number of pairs.

```{r, echo = FALSE, fig.align = "center", fig.height = 6, fig.width = 7, message=FALSE, warning=FALSE}
set.seed(4)
qc_thresh <- 250
ntc_effective_samp_size <- sample_size_df |>
  filter(grna_group == "non-targeting") |>
  group_by(response_id, dataset) |>
  summarize(effective_samp_size = sum(n_nonzero_cells))
undercover_res_w_ess <- undercover_res |>
  left_join(ntc_effective_samp_size,
            by = c("response_id", "dataset")) |>
  mutate(pass_qc = effective_samp_size >= qc_thresh)

n_papalexi_not_pass_qc <- undercover_res_w_ess |>
  filter(dataset == "papalexi_eccite_screen_gene",
         !pass_qc) |> nrow()

to_plot <- undercover_res_w_ess |>
  group_by(dataset, pass_qc) |>
  sample_n(size = n_papalexi_not_pass_qc) |>
  ungroup() |>
  update_dataset_names()

to_plot |>
  ggplot(mapping = aes(y = p_value, col = pass_qc)) +
  stat_qq_points(ymin = 1e-9, size = 0.8) +
  facet_wrap(~dataset_rename_w_pairs, scales = "free", ncol = 2, labeller = label_wrap_gen(35)) +
  geom_abline(col = "darkred") +
  stat_qq_band() +
  theme_bw() +
  scale_x_continuous(trans = revlog_trans(10)) +
  scale_y_continuous(trans = revlog_trans(10)) +
  labs(x = "Expected quantile", y = "Observed quantile") +
  theme(legend.position = "bottom",
       panel.grid.major = element_blank(),
       panel.grid.minor = element_blank(),
       panel.background = element_blank()) +
  labs(col="Passed QC")
```
We see that QC makes an enormous difference on the Frangieh data: the pairs that survive QC are fairly well calibrated, while those that are filtered out are poorly calibrated. By contrast, QC does not make much of a difference on the Papalexi data: the pairs that pass QC and those that do not are both fairly well-calibrated.

Next, I make a similar figure, this time plotting pairs with a "good" skew-normal fit alongside those with a "bad" (i.e., not "good") skew-normal fit. Again, I downsample so that each dataset-skew-normal fit status (i.e., "good" versus "bad") tuple contains the same number of pairs. Note that the number of pairs plotted below is much smaller than the number of pairs plotted above; thus, we are not looking as far out into the tail of the distribution. Still, the trend is clear: SCEPTRE exhibits similar performance on the Papalexi data regardless of whether the skew-normal fit is good or bad. By contrast, on the Frangieh data, a good skew-normal fit is required for p-value calibration.

```{r, message=FALSE, warning=FALSE, fig.height = 6, fig.width = 7, echo = FALSE, fig.align = "center"}
min_card <- sceptre_debug |>
  group_by(ks_fit_simple, dataset) |>
  summarize(count = n()) |>
  pull(count) |>
  min()

sceptre_debug |>
  group_by(dataset, ks_fit_simple) |>
  sample_n(min_card) |>
  update_dataset_names() |>
  ggplot(mapping = aes(y = p_value, col = ks_fit_simple)) +
  stat_qq_points(ymin = 1e-9, size = 0.8) +
  facet_wrap(~dataset_rename_w_pairs, scales = "free", ncol = 2, labeller = label_wrap_gen(35)) +
  geom_abline(col = "darkred") +
  stat_qq_band() +
  theme_bw() +
  scale_x_continuous(trans = revlog_trans(10)) +
  scale_y_continuous(trans = revlog_trans(10)) +
  labs(x = "Expected quantile", y = "Observed quantile") +
  theme(legend.position = "bottom",
       panel.grid.major = element_blank(),
       panel.grid.minor = element_blank(),
       panel.background = element_blank()) +
  labs(col="Skew-normal fit")
```

These two sets of results are concordant: on the Frangieh and Papalexi datasets, almost all of the variance in skew-normal fit quality is due to the effective sample size. Thus, pairs that pass QC tend to exhibit a good skew-normal fit, and conversely, pairs that are filtered tend to exhibit a bad skew-normal fit. In conclusion, a large effective sample size (or, roughly equivalently, a good skew-normal fit) is required for SCEPTRE to exhibit good calibration on the Frangieh data but not on the Papalexi data.

# Part 2: Why do the Papalexi and Frangieh datasets differ in this way?

I attempt to answer the following questions: Why on the Papalexi data does effective sample size not impact calibration quality? Put differently, how can pairs with a low effective sample size (and thus a poor skew-normal fit) exhibit good calibration? And why does this phenomenon not hold on the Frangieh data?

First, to get our bearings, I plot some of the SCEPTRE resampling distributions on the Papalexi data that fall into the "good," "adequate," "subpar," and "poor" categories. Here are four pairs in the **good** category:

```{r, include=FALSE, fig.align = "center"}
set.seed(10)
n_categories <- length(ks_stat_categories)
data_l <- c("papalexi/eccite_screen/gene", "frangieh/control/gene")

x <- lapply(data_l, function(dataset) {
  cowplot_lists <- vector(mode = "list", length = n_categories)
  names(cowplot_lists) <- names(ks_stat_categories)
  for (category in names(ks_stat_categories)) {
    curr_range <- ks_stat_categories[[category]]
    x <- sceptre_debug |>
    filter(dataset_slash == !!dataset) |>
    filter(ks_stat >= curr_range[1], ks_stat < curr_range[2])
    if (category == "poor") {
      n_plots <- 8
      sceptre_debug_sub <- x |> filter(p_rat > 2, p_value < 0.1) |> sample_n(n_plots)
    } else {
      n_plots <- 4
      sceptre_debug_sub <- x |> sample_n(n_plots)
    }
    ps <- lapply(X = seq(1, n_plots), FUN = function(i) {
      response_id <- sceptre_debug_sub[[i, "response_id"]] |> as.character()
      undercover_grna <- sceptre_debug_sub[[i, "undercover_grna"]] |> as.character()
      ks_stat <- sceptre_debug_sub[[i, "ks_stat"]]
      sceptre_args <- lowmoi::get_sceptre_function_args_for_pair(
      response_id = response_id, undercover_grna = undercover_grna,
      dataset_name = dataset, output_amount = 3, B = 2500)
      out <- do.call(what = sceptre2::run_sceptre_low_moi, args = sceptre_args)
      p_sn <- round(sceptre_debug_sub[i, "p_value"], 3)
      s_emp <- round(sceptre_debug_sub[i, "p_value_emp"], 3)
      tit <- paste0("KS = ", round(ks_stat, 3), "; p-SN = ", p_sn, "; p-Emp = ", s_emp)
      sceptre2:::plot_fitted_density_result_row(out, legend = FALSE, n_bins = 50) + ggtitle(tit)
  })
    cowplot_lists[[category]] <- cowplot::plot_grid(plotlist = ps, ncol = 2)
  }
  return(cowplot_lists)
}) |> setNames(c("papalexi", "frangieh"))
```

```{r, echo=FALSE, fig.align = "center", fig.height = 5, fig.width = 8}
plot(x$papalexi$good)
```

Here are four in the **adequate** category:

```{r, echo=FALSE, fig.align = "center", fig.height = 5, fig.width = 8}
plot(x$papalexi$adequate)
```

Here are four in the **subpar** category.

```{r, echo=FALSE, fig.align = "center", fig.height = 5, fig.width = 8}
plot(x$papalexi$subpar)
```

Finally, here are eight in the **poor** category.

```{r, echo=FALSE, fig.align = "center", fig.height = 10, fig.width = 8}
plot(x$papalexi$poor)
```

For comparison, I plot eight resampling distributions in the **poor** category from the Frangieh co-culture data.

```{r, echo=FALSE, fig.align = "center", fig.height = 10, fig.width = 8}
plot(x$frangieh$poor)
```

The Papalexi null distributions are less "lumpy" and multimodal than the Frangieh null distributions. Inflation occurs when there is a "lump" far out into the tail of the distribution, and this seems to be a more common occurrence on the Frangieh data than on the Papalexi data.

Next, I compute $p_{rat}$, the ratio of the skew-normal p-value to the empirical p-value on the Frangieh and Papalexi datasets. $p_{rat}$ quantifies the extent to which a skew-normal p-value is miscalibrated relative to the exact (permutation-based) p-value, with values larger than one indicating inflation. I downsample the Frangieh datasets so that all datasets have the same number of pairs to facilitate comparisons.

```{r, echo = FALSE, fig.align = "center", fig.height = 5, fig.width = 8}
x <- sceptre_debug |>
  filter(p_rat < 1e3, p_rat > 1e-3,
         dataset %in% c("frangieh_co_culture_gene",
                        "frangieh_control_gene",
                        "frangieh_ifn_gamma_gene",
                        "papalexi_eccite_screen_gene"))
n_papalexi <- x |>
  filter(dataset == "papalexi_eccite_screen_gene") |> nrow()
to_plot <- x |>
  group_by(dataset) |>
  sample_n(n_papalexi) |>
  update_dataset_names()

ggplot(data = to_plot, mapping = aes(x = ks_stat, y = p_rat, col = ks_fit)) +
  facet_wrap(. ~ dataset_rename_w_pairs, labeller = label_wrap_gen(35)) +
  geom_point(cex = 0.5, alpha = 0.6) +
  scale_y_log10() +
  scale_x_log10() +
  theme_bw() +
  ylab("p-rat") +
  guides(colour = guide_legend(override.aes = list(alpha = 1, cex = 1))) +
  xlab("KS statistic") +
  geom_hline(yintercept = 1) + 
  labs(col = "SN Fit")
```

The Papalexi data do not have many points far above one, whereas the Frangieh data do. Furthermore, the Frangieh data have many more "poor" pairs than Papalexi. Next, I condition on "bad" (i.e., not "good") pairs and compare across datasets, again downsampling to enable comparisons.

```{r, echo=FALSE, fig.align = "center", fig.height = 5, fig.width = 8}
set.seed(4)
x <- sceptre_debug |>
  filter(p_rat < 1e2, p_rat > 1e-2,
         dataset %in% c("frangieh_co_culture_gene",
                        "frangieh_control_gene",
                        "frangieh_ifn_gamma_gene",
                        "papalexi_eccite_screen_gene"),
         ks_fit != "good")
n_papalexi <- x |>
  filter(dataset == "papalexi_eccite_screen_gene") |>
  nrow()
to_plot <- x |>
  group_by(dataset) |>
  sample_n(n_papalexi) |>
  update_dataset_names()

ggplot(data = to_plot, mapping =  aes(x = ks_stat, y = p_rat, col = ks_fit)) +
  facet_wrap(. ~ dataset_rename_w_pairs, labeller = label_wrap_gen(35)) +
  geom_point(cex = 0.5, alpha = 0.6) +
  scale_y_log10() +
  scale_x_log10() +
  theme_bw() +
  ylab("p-rat") +
  guides(colour = guide_legend(override.aes = list(alpha = 1, cex = 1))) +
  xlab("KS statistic") +
  geom_hline(yintercept = 1) + 
  labs(col = "SN Fit")
```

The trend is similar: conditional on the event that a pair exhibits a "bad" skew-normal fit, a Frangieh pair is more likely to exhibit severe inflation than a Papalexi pair. Finally, for all datasets, and for a given value of $p_{\textrm{rat}}$ (e.g., $p_{\textrm{rat}} = 2$), I compute the fraction of "bad" pairs that falls above $p_{\textrm{rat}}$. The results are plotted below. The line corresponding to the Papalexi data (i.e., the purple line) falls far below the lines corresponding to the Frangieh data. This confirms that, conditional on the event that a pair exhibits a "bad" skew-normal fit, a Frangieh pair is much more likely to exhibit severe inflation than a Papalexi pair.

```{r, echo=FALSE, fig.align = "center", fig.height = 4, fig.width = 6}
# fraction of points below a given p-rat value
frac_above_df <- to_plot |>
  group_by(dataset_rename_w_pairs) |>
  group_modify(.f = function(tbl, key) {
    p_rat <- tbl$p_rat
    x_grid <- seq(1, 5, 0.05)
    y <- 1 - ecdf(p_rat)(x_grid)
    data.frame(y = y, x_grid = x_grid)
  })

ggplot(data = frac_above_df |>
         mutate(Dataset = dataset_rename_w_pairs),
       mapping = aes(x = x_grid,
                     y = y,
                     col = Dataset)) +
  geom_line(lwd = 1.1) +
  theme_bw() +
  xlab("p-rat") +
  ylab("Fraction pairs above p-rat")
```

We can answer the two questions posed at the top of this writeup as follows.

1. Question: Why is SCEPTRE reasonably well-calibrated on the Papalexi data but not on the Frangieh data?

Answer: Briefly, using a skew-normal approximation to the null distribution causes more problems on the Frangieh data than on the Papalexi data. We have the following simple model for the probability that a given pair exhibits inflation:

$$\mathbb{P}(\textrm{inflation}) = \mathbb{P}(\textrm{inflation} | \textrm{poor skew-normal fit}) \mathbb{P}(\textrm{poor skew-normal fit}).$$
Both quantities on the right-hand side of the equation are substantially larger on the Frangieh data than they are on the Papalexi data.

2. Question: Why does applying QC improve the quality of SCEPTRE on the Papalexi data but not on the Frangieh data?

Answer: Inflation (or lack thereof) is closely tied to skew-normal fit quality on the Frangieh data but not on the Papalexi data. This discrepancy comes down to differences in the shapes of the resampling distributions across datasets, with Frangieh resampling distributions being more likely to exhibit "lumps" far out into the tail.

# Part 3: What is going on with Seurat DE?

```{r}
set.seed(4)
qc_thresh <- 500

undercover_res_w_ess <- undercover_res_sde |>
  left_join(y = ntc_effective_samp_size,
            by = c("response_id", "dataset")) |>
  mutate(pass_qc = effective_samp_size >= qc_thresh)
  
samp_n <- undercover_res_w_ess |>
  group_by(pass_qc, dataset) |>
  summarize(count = n()) |>
  pull(count) |> min()

to_plot <- undercover_res_w_ess |>
  group_by(pass_qc, dataset) |>
  sample_n(samp_n)

to_plot |>
  update_dataset_names() |>
  ggplot(mapping = aes(y = p_value, col = pass_qc)) +
  stat_qq_points(ymin = 1e-9, size = 0.8) +
  facet_wrap(~dataset_rename_w_pairs, scales = "free", ncol = 2, labeller = label_wrap_gen(35)) +
  geom_abline(col = "darkred") +
  stat_qq_band() +
  theme_bw() +
  scale_x_continuous(trans = revlog_trans(10)) +
  scale_y_continuous(trans = revlog_trans(10)) +
  labs(x = "Expected quantile", y = "Observed quantile") +
  theme(legend.position = "bottom",
       panel.grid.major = element_blank(),
       panel.grid.minor = element_blank(),
       panel.background = element_blank()) +
  labs(col="Passed QC")
```

I applied Seurat DE to a subset of the Papalexi and Frangieh data, obtaining (pretty much) exact p-values by permuting the data and computing the Mann-Whitney (MW) test statistic B=100,000 times. I assessed the quality of the standard normal approximation to the null distribution by running a KS test on the resampled test statistics. I plot examples of empirical null distributions in the "good," "adequate," "subpar," and "poor" categories.

```{r, echo=FALSE}
# update the calculation of the empirical p-value using the jittered statistics (if necessary)
z_star <- mw_debug$z_star
null_stat_mat <- mw_debug |>
  select(starts_with("z_")) |>
  select(-z_star) |>
  as.matrix()

p_emp <- sapply(X = seq_along(z_star), FUN = function(i) {
  curr_z_star <- z_star[i]
  curr_z_null <- null_stat_mat[i,] + runif(n = length(row), min = -1e-5, max = 1e-5)
  p_emp <- sceptre2:::compute_empirical_p_value(z_star = curr_z_star,
                                                z_null = curr_z_null,
                                                side = "both")
  })
rm(null_stat_mat)
mw_debug$p_emp <- p_emp

# add the KS fit category
mw_debug$ks_fit <- cut(x = mw_debug$ks_stat,
                       breaks = unique(unlist(ks_stat_categories)),
                       labels = names(ks_stat_categories))
```

```{r, echo=FALSE}
plot_mw_debug_row <- function(row) {
  null_dist <- row |>
    select(-z_star) |>
    select(starts_with("z_")) |>
    as.numeric()
  x_grid <- seq(-4, 4, length.out = 1001)
  y_norm <- dnorm(x_grid)
  
  p <- ggplot() +
    geom_histogram(data = data.frame(null_dist = null_dist),
                   mapping = aes(x = null_dist, y = ..density..),
                   bins = 35, col = "black", fill = "gray") +
    geom_line(data = data.frame(x = x_grid, y = y_norm),
              mapping = aes(x = x, y = y), col = "darkred", lwd = 0.8) +
    theme_bw() +
    geom_vline(xintercept = row$z_star, col = "darkblue", lwd = 0.7) +
    xlab("") +
    ylab("")
  
  ks_stat <- row$ks_stat
  mw_p <- row$p_value
  emp_p <- row$p_emp
  
  tit <- paste0("KS = ", round(ks_stat, 3), "; p-MW = ", round(mw_p, 3), "; p-Emp = ", round(emp_p, 3))
  p <- p + ggtitle(tit)
  return(p)
}
```

```{r, message = FALSE, echo = FALSE}
mw_debug_papa <- mw_debug |>
  filter(dataset == "papalexi_eccite_screen_gene")
```

```{r, message = FALSE, echo = FALSE}
ks_fit_levels <- names(ks_stat_categories)
n_plots <- 4
ps <- lapply(ks_fit_levels, function(ks_stat_category) {
  x <- mw_debug_papa |>
    filter(ks_fit == !!ks_stat_category) |>
    sample_n(n_plots)
  p_list <- lapply(X = seq(1, n_plots), FUN = function(i) {
    plot_mw_debug_row(x[i,])
  })
  out <- cowplot::plot_grid(plotlist = p_list, ncol = 2)
}) |> setNames(ks_fit_levels)
```

Here are four in the "good" category.

```{r, echo=FALSE, fig.align = "center", fig.height = 5, fig.width = 8}
plot(ps$good)
```

Here are four in the "adequate" category.

```{r, echo=FALSE, fig.align = "center", fig.height = 5, fig.width = 8}
plot(ps$adequate)
```

Here are four in the "subpar" category.

```{r, echo=FALSE, fig.align = "center", fig.height = 5, fig.width = 8}
plot(ps$subpar)
```

And here are four in the "poor" category.

```{r, echo=FALSE, fig.align = "center", fig.height = 5, fig.width = 8}
plot(ps$poor)
```

Next, I study the relationship between standard normal fit quality and effective sample size. I plot standard normal fit quality (as quantified by KS statistic) against effective sample size for a set of pairs sampled from the Frangieh and Papalexi datasets.

```{r, message = FALSE, echo = FALSE, fig.align = "center"}
ntc_effective_samp_size <- sample_size_df |>
  filter(grna_group == "non-targeting") |>
  group_by(response_id, dataset) |>
  summarize(effective_samp_size = sum(n_nonzero_cells))

mw_debug_select <- mw_debug |>
  select(p_value, p_emp, ks_stat, dataset, undercover_grna, response_id, ks_fit) |>
  mutate(p_rat = p_value/p_emp) |>
 left_join(y = ntc_effective_samp_size,
           by = c("response_id", "dataset"))  |>
  mutate(method = "MW (perm)") |>
  update_dataset_names()

p <- mw_debug_select |>
  ggplot(mapping = aes(x = effective_samp_size, y = ks_stat)) +
  geom_point() +
  facet_wrap(dataset_rename_w_pairs~., scales = "free", labeller = label_wrap_gen(35)) +
  theme_bw() +
  scale_x_log10() +
  xlab("Effective sample size") +
  ylab("KS statistic")
p
```

We see that as effective sample size increases, the KS statistic decreases, indicating improved fit quality. Next, I plot p$_\textrm{rat}$ against the KS statistic.

```{r}
mw_debug_select |>
  ggplot(mapping = aes(x = ks_stat,
                       y = p_rat,
                       col = ks_fit)) +
  geom_point() +
  facet_wrap(dataset_rename_w_pairs ~ .,
             scales = "free",
             labeller = label_wrap_gen(35)) +
  theme_bw() +
  scale_x_log10() +
  scale_y_log10() + 
  xlab("KS statistic") +
  ylab("p-rat")


mw_debug_select |>
  ggplot(mapping = aes(x = effective_samp_size,
                       y = p_rat,
                       col = ks_fit)) +
  geom_point() +
  facet_wrap(dataset_rename_w_pairs ~ .,
             scales = "free",
             labeller = label_wrap_gen(35)) +
  theme_bw() +
  scale_x_log10() +
  scale_y_log10() + 
  xlab("Effective sample size") +
  ylab("p-rat")
```

What is going on with those pairs that exhibit miscalibration? They do not have small effective sample sizes in all cases. Therefore, what is going wrong?

```{r}
problem_pairs <- undercover_res_w_ess |>
  filter(p_value < 1e-3, dataset == "papalexi_eccite_screen_gene") |>
  select(response_id, undercover_grna) |>
  mutate(dataset = "papalexi/eccite_screen/gene",
         pair_status = "problem")

ok_pairs <- undercover_res_w_ess |>
  sample_n(size = nrow(problem_pairs)) |>
   select(response_id, undercover_grna) |>
  mutate(dataset = "papalexi/eccite_screen/gene",
         pair_status = "ok")

followup_pairs <- rbind(problem_pairs, ok_pairs)
saveRDS(object = followup_pairs, file = paste0(result_dir,
                                               "undercover_grna_analysis/mw_debug_pairs.rds"))
```
