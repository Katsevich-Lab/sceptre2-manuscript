---
title: "Statistical debugging of `sceptre` part 1: goodness of the skew-normal fit"
author: "Tim"
date: "2022-09-03"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# load packages
devtools::load_all("~/research_code/sceptre2/")
devtools::load_all("~/research_code/lowmoi/")
library(mgcv)
library(tidyverse)
reverselog_trans <- function(base = exp(1)) {
    trans <- function(x) -log(x, base)
    inv <- function(x) base^(-x)
    scales::trans_new(paste0("reverselog-", format(base)), trans, inv, 
              scales::log_breaks(base = base), 
              domain = c(1e-100, Inf))
}

# load results
sceptre_debug <- readRDS("/Users/timbarry/research_offsite/projects/sceptre2/results/undercover_grna_analysis/sceptre_debug.rds")

# define the categories
ks_stat_categories <- list(good = c(0, 1e-2),
                           adequate = c(1e-2, 5e-2),
                           subpar = c(5e-2, 0.1),
                           poor = c(0.1, 1))

# make some modifications to results
sceptre_debug <- sceptre_debug |>
  mutate(p_gap = abs(p_value - p_value_emp),
         p_rat = p_value_emp/p_value,
         ks_fit = cut(x = ks_stat,
                      breaks = unique(unlist(ks_stat_categories)),
                      labels = names(ks_stat_categories))) |>
  filter(dataset != "papalexi/eccite_screen/protein")
```

`sceptre` currently is not calibrated on low MOI single-cell CRISPR screen data. The objective of this analysis is to figure out why. I applied `sceptre` (within the context of the the undercover pipeline) to 500 randomly-selected genes from each dataset. I used `B = 100,000` permutations to enable precise comparisons between the "true" permutation p-value and the approximate permutation p-value based on the skew-normal fit. I analyzed the results and made three main discoveries: (i) the KS statistic is a good metric for skew-normal fit quality; (ii) a good skew-normal fit is required to produce an accurate p-value; and (iii) "effective sample size" --- in particular the number of control cells with a nonzero expression level --- is the primary driver of skew-normal fit quality.

## The KS statistic is a good single-number summary of skew-normal quality of fit

First, I verified that the KS statistic is a good single-number summary of skew-normal fit quality. Based on visual inspection of many resampling distributions, I partitioned the KS statistics into four categories summarizing goodness of fit: good, adequate, subpar, and poor (see table below).

| KS statistic range          | Categorization |
|-----------------------------|----------------|
| 0 $\leq$ KS stat \< 0.01    | Good           |
| 0.01 $\leq$ KS stat \< 0.05 | Adequate       |
| 0.05 $\leq$ KS stat \< 0.1  | Subpar         |
| 0.1 $\leq$ KS stat          | Poor           |

```{r, include=FALSE,cache=TRUE}
set.seed(4)
n_categories <- length(ks_stat_categories)
cowplot_lists <- vector(mode = "list", length = n_categories)
names(cowplot_lists) <- names(ks_stat_categories)

for (category in names(ks_stat_categories)) {
  curr_range <- ks_stat_categories[[category]]
  sceptre_debug_sub <- sceptre_debug |>
    filter(dataset == "frangieh/co_culture/gene") |>
    filter(ks_stat >= curr_range[1], ks_stat < curr_range[2]) |>
    sample_n(4)
  ps <- lapply(X = seq(1, 4), FUN = function(i) {
    response_id <- sceptre_debug_sub[i, "response_id"] |> as.character()
    undercover_grna <- sceptre_debug_sub[i, "undercover_grna"] |> as.character()
    ks_stat <- sceptre_debug_sub[i, "ks_stat"]
    sceptre_args <- get_sceptre_function_args_for_pair(
      response_id = response_id, undercover_grna = undercover_grna,
      dataset_name = "frangieh/co_culture/gene", output_amount = 3, B = 2500)
    out <- do.call(what = sceptre2::run_sceptre_low_moi, args = sceptre_args)
    tit <- paste0("KS = ", round(ks_stat, 5))
    sceptre2::plot_fitted_density_result_row(out, legend = FALSE) + ggtitle(tit)
  })
  cowplot_lists[[category]] <- cowplot::plot_grid(plotlist = ps)
}
```

Let us look at four randomly-selected resampling distributions from each of these categories on the Frangieh co-culture gene expression data. Here are four undercover gRNA-gene pairs in the **good** category.

```{r, echo=FALSE}
plot(cowplot_lists$good)
```

Next, here are four in the **adequate** category.

```{r, echo=FALSE}
plot(cowplot_lists$adequate)
```

Next, here are four in the **subpar** category.

```{r, echo=FALSE}
plot(cowplot_lists$subpar)
```

Finally, here are four in the **poor** category.

```{r, echo=FALSE}
plot(cowplot_lists$poor)
```

The KS statistic appears to closely track fit quality. Also, as the fit gets worse, the skew-normal fit deviates more from the N(0,1) distribution. I computed the fraction of pairs falling into each category for each dataset, presented below.

```{r, echo=FALSE}
skew_normal_fit <- sceptre_debug |>
  group_by(dataset) |>
  group_modify(.f = function(tab, key) {
    tab <- prop.table(table(tab$ks_fit))
    data.frame(good = tab[1], adequate = tab[2], subpar = tab[3], poor = tab[4])
  })
skew_normal_fit_m <- as.matrix(skew_normal_fit[,2:5])
#colnames(skew_normal_fit_m) <- paste0(colnames(skew_normal_fit_m))
#rownames(skew_normal_fit_m) <- skew_normal_fit$dataset
colnames(skew_normal_fit_m) <- rep("", ncol(skew_normal_fit_m))
rownames(skew_normal_fit_m) <- rep("", nrow(skew_normal_fit_m))
skew_normal_fit |> dplyr::mutate_at(.vars = c("good", "adequate", "subpar", "poor"), ~ round(., 3)) |> knitr::kable()
```

I also created a mosaic plot to visualize these results. We see that "good" is the most common category, although there is a fair amount of variation across datasets.

Color legend:

-   black = "good"

-   dark grey = "adequate"

-   light grey = "subpar"

-   white = "poor."

Datasets from left to right:

-   frangieh/co_culture/gene

-   frangieh/control/gene

-   schraivogel/ground_truth_tapseq/gene

-   schraivogel/enhancer_screen_chr11/gene

-   frangieh/ifn_gamma/gene

-   schraivogel/enhancer_screen_chr8/gene

-   papalexi/eccite_screen/gene

-   schraivogel/ground_truth_perturbseq/gene

-   simulated/experiment_1/gene.

```{r, echo=FALSE}
mosaicplot(skew_normal_fit_m, color = TRUE, main = "Mosaic plot of KS statistic fits")
```

Despite the fact that "good" is the most common category, subpar and poor skew-normal fits appear to be more common than perhaps we had expected.

## A good skew-normal fit is required for accurate p-value approximation

I next investigated the relationship between goodness of the skew-normal fit (as quantified by the KS statistic) and accuracy of the skew-normal p-value. I calculated the discrepancy between the "true" empirical p-value $p_{emp}$ (carried out to $B = 100,000$ replicates) and the skew-normal p-value $p_{sn}$. I considered two measures of discrepancy: $p_{dif}$, the absolute value of the difference between the p-values (defined as $p_{dif} = |p_{emp} - p_{sn}|$), and $p_{rat}$, the ratio of the skew-normal p-value to the empirical p-value (defined as $p_{rat} = p_{sn}/p_{emp}$). The former measure is more meaningful for p-values in the bulk of the distribution, while the latter is more meaningful for p-values in the tail.

I plotted $p_{dif}$ and $p_{rat}$ against the KS statistic, coloring the pairs by fit quality ("good", "adequate", "subpar", and "poor") and faceting by dataset. First, examining $p_{dif}$, we see that as the skew-normal fit becomes worse, $p_{dif}$ increases in magnitude and become more variable.

```{r, echo = FALSE}
sceptre_debug_sub <- sceptre_debug |> 
  filter(p_value_emp > 100/100000)

ggplot(data = sceptre_debug_sub,
       mapping = aes(x = ks_stat, y = p_gap, col = ks_fit)) + 
  facet_wrap(. ~ dataset) +
  geom_point(cex = 0.3, alpha = 0.3) +
  scale_x_log10() +
  theme_bw() +
  guides(colour = guide_legend(override.aes = list(alpha = 1, cex = 1))) +
  ylab("p-dif") +
  xlab("Skew-normal fit quality") +
  labs(col = "SN Fit")
```

The corresponding plot for $p_{rat}$ is below. (The y-axis has been log-transformed. Also, $p_{rat}$ values of above 1,000 and below 0.001 have been excluded from the plot, causing many points in the "poor" category to not appear.)

Encouragingly, virtually every skew-normal p-value in the "good" category is well within an order of magnitude of the true empirical p-value. The same cannot be said about skew-normal p-values in the "poor" category.

```{r, echo = FALSE}
ggplot(data = sceptre_debug_sub |> filter(
  p_rat < 1e3, p_rat > 1e-3),
       mapping = aes(x = ks_stat, y = p_rat, col = ks_fit)) + 
  facet_wrap(. ~ dataset) +
  geom_point(cex = 0.5, alpha = 0.6) +
  scale_y_log10() +
  scale_x_log10() +
  theme_bw() +
  ylab("p-rat") +
  guides(colour = guide_legend(override.aes = list(alpha = 1, cex = 1))) +
  xlab("Skew-normal fit quality") +
  geom_hline(yintercept = 1) + 
  labs(col = "SN Fit")
```

Finally, I directly plotted the empirical p-values against the skew-normal p-values, putting the "good" and "poor" pairs into different figures (while ignoring the "adequate" and "subpar" pairs for simplicity) and faceting by dataset. The plot below shows the "good" pairs. We see fairly strong agreement between the empirical and skew-normal p-values, with mild discordance for the Frangieh datasets. Scales are negative log 10 transformed to accentuate the tails of the distributions.

```{r, echo = FALSE}
ggplot(data = sceptre_debug_sub |> filter(p_value > 1e-8, ks_fit == "good"),
       mapping = aes(p_value, p_value_emp)) +
  geom_point(cex = 0.5, alpha = 0.5) +
  geom_abline(intercept = 0, slope = 1, col = "darkblue", lwd = 0.8) +
  facet_wrap(.~dataset) +
  theme_bw() +
  xlab("Skew-normal p") + ylab("Emprical p") +
  scale_x_continuous(trans=reverselog_trans(10)) +
  scale_y_continuous(trans=reverselog_trans(10))
```

The next plot shows the empirical and skew-normal p-values of the "poor" pairs. The disagreement between these two sets of p-values is much more extreme here.

```{r, echo = FALSE}
ggplot(data = sceptre_debug_sub |> filter(p_value > 1e-8, ks_fit == "poor"),
       mapping = aes(p_value, p_value_emp)) +
  geom_point(cex = 0.5, alpha = 0.5) +
  geom_abline(intercept = 0, slope = 1, col = "darkblue", lwd = 0.8) +
  facet_wrap(.~dataset) +
  theme_bw() +
  xlab("Skew-normal p") + ylab("Emprical p") +
  scale_x_continuous(trans=reverselog_trans(10)) +
  scale_y_continuous(trans=reverselog_trans(10))
```

Pairs above the blue line have a skew-normal p-value that is **larger** (i.e., less significant) than the empirical p-value. Pairs below the blue line, by contrast, have a skew-normal p-value that is **smaller** (i.e., more significant) than the empirical p-value. Pairs below the blue line therefore are more concerning than pairs above, and it is clear that pairs with a "poor" skew-normal fit are much more likely to fall far below the blue line than pairs with a "good fit." We conclude that a poor skew-normal fit is more likely to result in a grossly inflated p-value than a good skew-normal fit.

Overall, these analyses indicate (perhaps unsurprisingly) that the quality of the skew-normal fit matters: good skew-normal fits lead to reasonably good p-value approximations, while poor skew-normal fits lead to poor approximations.

## Small effective sample size leads to poor skew-normal fit

I next sought to understand why the skew-normal fit sometimes is poor. I hypothesized that there are three main determinants of goodness of the skew-normal fit: (i) the number of "treatment" cells (i.e., cells that receive a targeting gRNA) with non-zero gene expression, (ii) the number of "control" cells (i.e., cells that receive a non-targeting gRNA) with non-zero gene expression, and (iii) goodness of the negative binomial GLM fit to the data. The test statistic that `sceptre` uses is a score statistic derived from a distilled negative binomial GLM. When the "effective sample size" is large, and when the negative binomial GLM fits the data well, the permuted test statistics form a $N(0,1)$ distribution. (I am pretty certain that this can be proven, but I have not yet done so.) As we deviate from this ideal setting, the permuted test statistics deviate from the theoretical $N(0,1)$ distribution, causing the skew-normal fit to degrade.

I therefore empirically tested the hypothesis that "effective sample size" (as quantified by the number of treatment cells, N treatment cells, and the number of control cells, N control cells) impacts quality of the skew-normal fit (as quantified by the KS statistic). First, I plotted the KS statistic against N treatment cells, log-transforming both *x* and *y* axes. The trend is clear: as N treatment cells increases, the KS statistic decreases (signifying better skew-normal fit to the empirical distribution of permuted statistics).

```{r, echo = FALSE}
ggplot(data = sceptre_debug,
       mapping = aes(x = n_treatment_cells_with_expression + 1,
                     y = ks_stat)) +
  geom_point(cex = 0.5, alpha = 0.5) +
  scale_x_log10() +
  scale_y_log10() +
   facet_wrap(.~dataset) +
  theme_bw() +
  xlab("N treatment cells + 1") +
  ylab("KS statistic")
```

I created a similar plot for N control cells (below), and the results were even more striking: the KS statistic and N control cells exhibited a consistent strong and negative (albeit nonlinear) association.

```{r, echo = FALSE}
ggplot(data = sceptre_debug,
       mapping = aes(x = n_control_cells_with_expression + 1,
                     y = ks_stat)) +
  geom_point(cex = 0.5, alpha = 0.5) +
  scale_x_log10() +
  scale_y_log10() +
   facet_wrap(.~dataset) +
  theme_bw() +
  xlab("N control cells + 1") +
  ylab("KS statistic")
```

To quantify the extent to which N treatment cells and N control cells explain variation in the KS statistic, I fitted the following three GAMs to each dataset:

$$
\begin{cases}
\textrm{log}(\textrm{KS statistic}) \sim s(\textrm{log}(\textrm{N treatment cells})) \\
\textrm{log}(\textrm{KS statistic}) \sim s(\textrm{log}(\textrm{N control cells})) \\
\textrm{log}(\textrm{KS statistic}) \sim s(\textrm{log}(\textrm{N control cells})) + s(\textrm{log}(\textrm{N control cells})),
\end{cases}
$$

where $s$ denotes a spline. I assumed a Gaussian response and an identity link in all three cases. I extracted from the fitted models the percent of deviance (in the response) explained by the predictors. The results are below. We see that in general, N treatment cells and N control cells together explain a large fraction of the deviance in the response ($>85\%$ on all real datasets, except the Schraivogel enhancer screen 11 dataset). Individually, N control cells explains a larger fraction of the deviance than does N treatment cells. This analysis convincingly validates the hypothesis that "effective sample size" --- as measured by N treatment cells and N control cells --- controls quality of the skew-normal fit.

```{r, cache=TRUE, echo=FALSE, message=FALSE}
# Fitting GAMs to each; two questions: (i) fraction deviance explained?
# (ii) what sample size do we need (for both treated and untreated) to have a "good" fit?
gam_out <- sceptre_debug |>
  select(dataset,
         ks_stat,
         n_treatment_cells_with_expression,
         n_control_cells_with_expression) |>
 mutate(lg_ks_stat = log(ks_stat),
        lg_n_treatment = log(n_treatment_cells_with_expression + 1),
        lg_n_control = log(n_control_cells_with_expression + 1)) |>
  group_by(dataset) |>
  group_map(.f = function(tbl, key) {
    # fits
    fit_n_treatment <- gam(formula = lg_ks_stat ~ s(lg_n_treatment), data = tbl)
    fit_n_control <- gam(formula = lg_ks_stat ~ s(lg_n_control), data = tbl)
    fit_both <- gam(formula = lg_ks_stat ~ s(lg_n_treatment) + s(lg_n_control), data = tbl)
    
    # predict
    newdata <- data.frame(n_control = seq(1, 5000)) |>
      dplyr::mutate(lg_n_control = log(n_control + 1))
    pred_means <- predict(object = fit_n_control, newdata = newdata, se.fit = TRUE)        
    pred_df <- newdata |> dplyr::mutate(
      lg_ks = pred_means$fit,
      upper_ci = exp(pred_means$fit + 1.96 * pred_means$se.fit),
      lower_ci =  exp(pred_means$fit - 1.96 * pred_means$se.fit),
      ks = exp(lg_ks))
    
    # minimal value N control cells value for "good" skew-normal fit
    min_n_control <- pred_df$n_control[min(which(pred_df$ks < 0.01))]
    
    # deviance
    dev_explained_n_treatment <- summary(fit_n_treatment)$dev.expl
    dev_explained_n_control <- summary(fit_n_control)$dev.expl
    dev_explained_both <- summary(fit_both)$dev.expl
    
    list(dataset = as.character(key$dataset),
         dev_explained_n_treatment = dev_explained_n_treatment,
         dev_explained_n_control = dev_explained_n_control,
         dev_explained_both = dev_explained_both,
         min_n_control = min_n_control,
         pred_df = pred_df)
  })
```

```{r, echo=FALSE}
dev_explained <- data.frame(
  dataset = sapply(gam_out, function(elem) elem[["dataset"]]),
  dev_explained_n_treatment = sapply(gam_out, function(elem) elem[["dev_explained_n_treatment"]]),
  dev_explained_n_control = sapply(gam_out, function(elem) elem[["dev_explained_n_control"]]),
  dev_explained_both = sapply(gam_out, function(elem) elem[["dev_explained_both"]])) |>
  mutate_at(c("dev_explained_n_treatment",
              "dev_explained_n_control",
              "dev_explained_both"), ~round(., 3))

dev_explained |> knitr::kable()
```

Given that N control cells appears to be more important than N treatment cells in determining quality of the skew-normal fit, I sought to determine the (rough) number of control cells with nonzero expression required to obtain a "good" skew-normal fit (defined as KS statistic \< 0.01). To this end, I evaluated the (second of the three) fitted GAMs on a grid of values for N treatment cells and identified the minimal value of N treatment cells that rendered the average KS statistic below 0.01. These dataset-specific thresholds are as follows:

```{r, echo = FALSE}
min_n_control <- sapply(gam_out, function(elem) elem$min_n_control)
dataset_names <- sapply(gam_out, function(elem) elem$dataset)
min_n_control_df <- data.frame(dataset = dataset_names, n_control_cells_threshold = min_n_control)
min_n_control_df |> knitr::kable()
```

These thresholds number in the hundreds for the real datasets. Finally, I plotted the fitted mean (alongside pointwise 95% CIs) for each GAM. The horizontal red line indicates a KS statistic value of 0.01 (the boundary for a "good" fit), and the vertical black line indicates the value of N control cells for which a given dataset yields an (average) KS statistic of 0.01.

```{r, echo = FALSE}
to_plot <- lapply(gam_out, function(elem) {
  elem$pred_df |> 
    mutate(dataset = elem$dataset)
}) |> bind_rows()
ggplot(data = to_plot, mapping = aes(x = n_control + 1,
                                     y = ks,
                                     ymin = lower_ci,
                                     ymax = upper_ci)) +
  geom_ribbon(fill = "grey") +
  geom_line(col = "darkblue", lwd = 0.8) +
  geom_vline(mapping = aes(xintercept = n_control_cells_threshold),
             data = min_n_control_df) +
  facet_wrap(.~dataset) +
  theme_bw() +
  scale_x_log10() +
  scale_y_log10() +
  ylab("KS statistic") +
  xlab("N control cells + 1") +
  geom_hline(yintercept = 0.01, col = "darkred")
```

## Closing thoughts

In this writeup I demonstrated three main claims: (i) the KS statistic is a good metric for skew-normal fit quality; (ii) a good skew-normal fit is required for an accurate p-value; and (iii) "effective sample size" --- in particular the number of control cells with a nonzero expression level --- is the primary driver of skew-normal fit quality. Skew-normal fit becomes good when (conservatively) this value crosses about 500. I conclude with a few closing thoughts.

-   **Three core challenges of single-cell CRISPR screen analysis**. Our various analyses (including this one) have illuminated three core challenges to single-cell CRISPR screen analysis, shared across low and high MOI:

    -   Correctly specifying gene expression models.

    -   Accounting for technical covariates, both to control type-I error control and boost power.

    -   Sparsity/discreteness of the data.

    This writeup largely focused the third challenge. Sparsity renders the "effective sample size" small, leading to poor skew-normal fits and inaccurate (and potentially hugely inflated) p-values.

-   **Low vs. high MOI**. We have observed that sparsity is a greater challenge in low MOI than in high MOI, and now we know why: the number of *control* cells with a nonzero expression level drives effective sample size, and this quantity is much smaller in low MOI than it is in high MOI. This is because the control group in low MOI is the set of cells that received a non-targeting gRNA, whereas the control group in high MOI is the complement of the set of cells that received a targeting gRNA.

-   **A more sophisticated model for goodness of skew-normal fit**. In the above analysis I derived dataset-specific thresholds for producing a "good" skew-normal fit by fitting GAMs with a single predictor: N control cells. We might consider obtaining better thresholds by leveraging more sophisticated, multivariate models, based either on GAMs or other methods (e.g., ML algorithms).

-   **Leftover variability in the KS statistics**. I hypothesize that most of the "leftover" deviance in the KS statistics (i.e., the deviance not explained by the models) is due to variability in the quality of the negative binomial GLM fit. I intend on exploring this hypothesis in an updated version of this writeup.

-   **The effect of QC**. We applied fairly lax QC to the datasets that we analyzed (so as to put the competing methods into a "hard" problem setting). Presumably, as one applies more stringent QC, the problem of sparsity/discreteness diminishes.

-   **Connections to calibration**. Our ultimate goal of course is to propose a well-calibrated method. In my next writeup ("Part 2") I will ask and attempt to answer the following questions: Are the empirical p-values calibrated? Are skew-normal p-values that are computed from pairs with large effective sample size calibrated? Is the Fisher exact test calibrated? Can we synthesize these ideas to propose a unified, well-calibrated method in all problem settings?
