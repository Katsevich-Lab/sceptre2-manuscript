coef_ll <- as.vector(coef(lm.prodX, s = lm.prodX$lambda[ll]))[-1]
pred.epsX[, k] <- predict(lm.prodX, newx = Z, s = lm.prodX$lambda[ll])
if(length(which(coef_ll != 0)) > 0){
coef_prodX <- coef_ll
break
}
}
}
}
pred.resX[((n*(l-1) + 1):(n*l)), ] <- pred.epsX
}
# regress eps.Y on pred.epsX
if (p == 1){
aa <- res.X*X
lm.prodY <- lm(res.Y~pred.resX)
result_coef <- as.vector(coef(lm.prodY))[-1]
}else{
lm.prodY <- cv.glmnet(pred.resX, res.Y)
result_coef <- as.vector(coef(lm.prodY, s = lm.prodY$lambda.min))[-1]
}
# estimate the precision matrix P
# L <- sample(round((q*0.25),0):(q*0.3), 1)
if (q == 400){
L <- 100
}else{
L <- 100
}
eps <- Y - X*result_coef + L
eps_Z <- matrix(0, nrow = n, ncol = q)
for (j in 1:q) {
eps_Z[, j] <- Z[, j] * eps
}
P <- t(Z) %*% eps_Z / n
# use Kang's code to implement
prec_P <- InverseLinfty(P, n, resol=1, threshold = 1e-2, verbose = TRUE)
# debias for gamma
Sigma_z <- t(Z) %*% Z / n
prec_Lam <- InverseLinfty(Sigma_z, n, resol=1, threshold = 1e-2, verbose = TRUE)
gamma_tilde <- as.vector(coef(lm.X, s = lm.X$lambda.min))[-1] + prec_Lam %*% (t(Z) %*% res.X) / n
res.X_debias <- as.vector(X[, l] - (Z %*% gamma_tilde))
# debias for beta
divider <- (t(coef_prodX) %*% prec_P) %*% (n * Sigma_z %*% coef_prodX)
debias_add <- (t(coef_prodX) %*% prec_P ) %*% (t(Z) %*%(res.X_debias * (eps - L))) / divider
debias_coef <- result_coef + debias_add
# multiplier bootstrap
B <- 500
boot_list <- numeric(B)
for(b in 1:B){
boot_gauss <- rnorm(n)
boot_list[b] <- (t(coef_prodX) %*% prec_P ) %*% (t(Z) %*%(boot_gauss*res.X_debias * (eps - L)))
}
add_term <- quantile(sort(abs(boot_list)), 0.95) / divider
debias_ci <- c(debias_coef - add_term, debias_coef + add_term)
# error vector & analysis
l1_err[r] <- sum(abs(result_coef - beta))
debias_err[r] <- sum(abs(debias_coef - beta))
our_debias_ci[r, ] <- debias_ci
# print(abs(result_coef - beta))
print(sum(abs(debias_coef - beta)))
print(debias_ci)
if(beta > min(debias_ci) & beta < max(debias_ci)){
ll <- ll+1
}
print(ll/r)
# zijian's method
# guo <- TSHT(Y, X, Z, method="DeLasso")
# guo_debias[r] <- abs(guo$betaHat - beta)
# guo_debias_ci[r, ] <- guo$ci
# print(guo_debias[r])
#
# # next round if n <= q
# if(n <= q){
#   next
# }else{
#   # Ting Ye's method
#   ting <- mr.genius(z = Z, a = X, y = Y)
#   ting_debias_ci[r, ] <- ting$ci
#   ting_debias[r] <- abs(ting$beta.hat - beta)
#   print(ting_debias[r])
#   if(beta > min(ting$ci) & beta < max(ting$ci)){
#     tt <- tt + 1
#   }
#   print(tt/r)
# }
}
mainDir <- "/Users/ziang/Dropbox/Mac/Documents/Projects/InvalidIV/simulation"
subDir <- sprintf("%s_%d_%d", type,  n, q)
dir.create(file.path(mainDir, subDir))
write.csv(debias_err, sprintf("%s/%s", file.path(mainDir, subDir),  "our_debias.csv"))
write.csv(normalize_debias, sprintf("%s/%s", file.path(mainDir, subDir),  "our_normalize_debias.csv"))
write.csv(ting_debias, sprintf("%s/%s", file.path(mainDir, subDir),  "ting_debias.csv"))
write.csv(ting_debias_ci, sprintf("%s/%s", file.path(mainDir, subDir),  "ting_debias_ci.csv"))
write.csv(guo_debias, sprintf("%s/%s", file.path(mainDir, subDir),  "guo_debias.csv"))
write.csv(guo_debias_ci, sprintf("%s/%s", file.path(mainDir, subDir),  "guo_debias_ci.csv"))
}
}
# specify type of IV
type <- "weak"
n_list <- c(4000, 2000, 4000)
q_list <- c(200, 200)
data_gener<-function(m, n, beta0, gamma){
z_coef<-data.frame(z_x=rep(gamma,m))
z<-matrix(nrow=n,ncol=m)
for(j in 1:m){
z[,j]<- sample(3, n, replace = TRUE, prob = c(0.6, 0.2, 0.2)) - 1.6
}
coef_az <- (2*rbinom(m, 1, 0.5) - 1)
coef_ez <- (2*rbinom(100, 1, 0.5) - 1)
mag <- 0.08
u<-rnorm(n, 0, 0.48)
mag_y <- rnorm(1:m, sqrt(0.15/(n)), 0.15/(n))
# geneate treatment and outcome
a<-z %*% coef_az*sqrt(0.15/(n)) + u + rnorm(n, 0, 1)*(1 + (z[,1:100] %*% coef_ez*mag))
y<-beta0*a + z[,1:100] %*% rep(0.08, 100) + u +  rnorm(n, 0, 1)
return(list(z=z, a=a, y=y))
}
# data generating mechanism
data_generate <- function(n, p, q, s1, s2, s3, nu, mu, type){
if(type == "weak"){
beta <- mu
data_xyz <- data_gener(q, n, beta = beta, gamma = 10)
data <- list(Z = data_xyz$z, X = data_xyz$a, Y = data_xyz$y,
beta = beta)
return(data)
}else{
# generate Z and center it by subtracting the mean
Z <- matrix(0, nrow = n, ncol = q)
for (j in 1:q) {
Z[, j] <- sample(3, n, replace = TRUE, prob = c(0.6, 0.2, 0.2)) - 1.6
}
# unobserved confounding variable
U <- matrix(0, nrow = n, ncol = p)
for (l in 1:p) {
U[, l] <- rnorm(n)
}
# independent error for X model
xi <- matrix(0, nrow = n, ncol = p)
for (l in 1:p) {
xi[, l] <- rnorm(n)
}
# generate data coefficient matrix Gamma and treatment matrix X
Gamma <- matrix(0, nrow = q, ncol = p)
X <- matrix(0, nrow = n, ncol = p)
stre <- 2
for (l in 1:p) {
Gamma[sample(1:q, s1), l] <- nu*(2*rbinom(s1, 1, 0.5) - 1)
act_set <- which(Gamma[, l] != 0)
X[, l] <- Z %*% Gamma[,l] + 4*U[, l] + xi[, l]*((stre*(Z[, 1]))^3)
}
# generate outcome Y
eta <- rnorm(n)
alpha <- numeric(q)
beta <- numeric(p)
alpha[sample(1:q, s2)] <- 0.5*(2*rbinom(s2, 1, 0.5) - 1)
beta[sample(1:p, s3)] <- mu
Y <- Z %*% Gamma + X %*% beta +  4*U[,1] + eta
data <- list(Z = Z, X = X, Y = Y,
beta = beta)
return(data)
}
}
# run the simulation in n_list and q_list
for (i in 1:length(n_list)) {
for (j in 1:length(q_list)) {
ll <- 0
nd <- 0
tt <- 0
n <- n_list[i]
q <- q_list[j]
p <- 1
s1 <- 5
s2 <- q
s3 <- 1
nu <- 2
mu <- 1
error_vec <- matrix(0, nrow = p, ncol = 300)
l1_err <- numeric(300)
l2_err <- numeric(300)
debias_err <- numeric(300)
our_debias_ci <- matrix(0, nrow = 300, ncol = 2)
guo_debias_ci <- matrix(0, ncol = 2, nrow = 300)
ting_debias_ci <- matrix(0, ncol = 2, nrow = 300)
naive_debias_err <- numeric(300)
naive_debias_ci <- matrix(0, nrow = 300, ncol = 2)
for (r in 1:300) {
set.seed(r)
# generate data
data <- data_generate(n = n, p = p, q = q,
s1 = s1, s2 = s2, s3 = s3,
nu = nu, mu = mu, type = type)
X <- data$X
Z <- data$Z
Y <- data$Y
beta <- data$beta
# center Z and X
# Z <- scale(Z, scale = FALSE)
# X <- scale(X, scale = FALSE)
# estimation for X on Z predictions and regress eps.X on Z
res.X <- matrix(0, nrow = n, ncol = p)
pred.epsX <- matrix(0, nrow = n, ncol = p)
res.Y <- numeric(n*p)
pred.resX <- matrix(0, nrow = n*p, ncol = p)
post_coef <- matrix(0, nrow = q, ncol = p)
for (l in 1:p) {
lm.X <- cv.glmnet(Z, X[,l])
# lm.X <- RPtests::sqrt_lasso(Z, X[,l])
res.X[, l] <- X[, l] - predict(lm.X, newx = Z, s = lm.X$lambda.min)
res.Y[(n*(l-1) + 1):(n*l)] <- res.X[,l]*Y
for (k in 1:p) {
lm.prodX <- cv.glmnet(Z, res.X[, l]*X[, k])
coef_prodX <- as.vector(coef(lm.prodX, s = lm.prodX$lambda.min))[-1]
if(length(which(coef_prodX != 0)) > 0){
lm.prodX_post <- lm(res.X[, l]*X[, k] ~ Z[, which(coef_prodX != 0)])
post_coef[which(coef_prodX != 0), k] <- as.vector(coef(lm.prodX_post))[-1]
pred.epsX[, k] <- cbind(1, Z[, which(coef_prodX != 0)]) %*% as.vector(coef(lm.prodX_post))
coef_prodX <- post_coef
}else{
for (ll in 1:length(lm.prodX$lambda)) {
coef_ll <- as.vector(coef(lm.prodX, s = lm.prodX$lambda[ll]))[-1]
pred.epsX[, k] <- predict(lm.prodX, newx = Z, s = lm.prodX$lambda[ll])
if(length(which(coef_ll != 0)) > 0){
coef_prodX <- coef_ll
break
}
}
}
}
pred.resX[((n*(l-1) + 1):(n*l)), ] <- pred.epsX
}
# regress eps.Y on pred.epsX
if (p == 1){
aa <- res.X*X
lm.prodY <- lm(res.Y~pred.resX)
result_coef <- as.vector(coef(lm.prodY))[-1]
}else{
lm.prodY <- cv.glmnet(pred.resX, res.Y)
result_coef <- as.vector(coef(lm.prodY, s = lm.prodY$lambda.min))[-1]
}
# estimate the precision matrix P
# L <- sample(round((q*0.25),0):(q*0.3), 1)
if (q == 400){
L <- 100
}else{
L <- 100
}
eps <- Y - X*result_coef + L
eps_Z <- matrix(0, nrow = n, ncol = q)
for (j in 1:q) {
eps_Z[, j] <- Z[, j] * eps
}
P <- t(Z) %*% eps_Z / n
# use Kang's code to implement
prec_P <- InverseLinfty(P, n, resol=1, threshold = 1e-2, verbose = TRUE)
# debias for gamma
Sigma_z <- t(Z) %*% Z / n
prec_Lam <- InverseLinfty(Sigma_z, n, resol=1, threshold = 1e-2, verbose = TRUE)
gamma_tilde <- as.vector(coef(lm.X, s = lm.X$lambda.min))[-1] + prec_Lam %*% (t(Z) %*% res.X) / n
res.X_debias <- as.vector(X[, l] - (Z %*% gamma_tilde))
# debias for beta
divider <- (t(coef_prodX) %*% prec_P) %*% (n * Sigma_z %*% coef_prodX)
debias_add <- (t(coef_prodX) %*% prec_P ) %*% (t(Z) %*%(res.X_debias * (eps - L))) / divider
debias_coef <- result_coef + debias_add
# multiplier bootstrap
B <- 500
boot_list <- numeric(B)
for(b in 1:B){
boot_gauss <- rnorm(n)
boot_list[b] <- (t(coef_prodX) %*% prec_P ) %*% (t(Z) %*%(boot_gauss*res.X_debias * (eps - L)))
}
add_term <- quantile(sort(abs(boot_list)), 0.95) / divider
debias_ci <- c(debias_coef - add_term, debias_coef + add_term)
# error vector & analysis
l1_err[r] <- sum(abs(result_coef - beta))
debias_err[r] <- sum(abs(debias_coef - beta))
our_debias_ci[r, ] <- debias_ci
# print(abs(result_coef - beta))
print(sum(abs(debias_coef - beta)))
print(debias_ci)
if(beta > min(debias_ci) & beta < max(debias_ci)){
ll <- ll+1
}
print(ll/r)
# zijian's method
# guo <- TSHT(Y, X, Z, method="DeLasso")
# guo_debias[r] <- abs(guo$betaHat - beta)
# guo_debias_ci[r, ] <- guo$ci
# print(guo_debias[r])
#
# # next round if n <= q
# if(n <= q){
#   next
# }else{
#   # Ting Ye's method
#   ting <- mr.genius(z = Z, a = X, y = Y)
#   ting_debias_ci[r, ] <- ting$ci
#   ting_debias[r] <- abs(ting$beta.hat - beta)
#   print(ting_debias[r])
#   if(beta > min(ting$ci) & beta < max(ting$ci)){
#     tt <- tt + 1
#   }
#   print(tt/r)
# }
}
mainDir <- "/Users/ziang/Dropbox/Mac/Documents/Projects/InvalidIV/simulation"
subDir <- sprintf("%s_%d_%d", type,  n, q)
dir.create(file.path(mainDir, subDir))
write.csv(debias_err, sprintf("%s/%s", file.path(mainDir, subDir),  "our_debias.csv"))
write.csv(normalize_debias, sprintf("%s/%s", file.path(mainDir, subDir),  "our_normalize_debias.csv"))
write.csv(ting_debias, sprintf("%s/%s", file.path(mainDir, subDir),  "ting_debias.csv"))
write.csv(ting_debias_ci, sprintf("%s/%s", file.path(mainDir, subDir),  "ting_debias_ci.csv"))
write.csv(guo_debias, sprintf("%s/%s", file.path(mainDir, subDir),  "guo_debias.csv"))
write.csv(guo_debias_ci, sprintf("%s/%s", file.path(mainDir, subDir),  "guo_debias_ci.csv"))
}
}
knitr::opts_chunk$set(echo = TRUE)
library(eva)
library(tidyverse)
library(katlabutils)
library(cowplot)
library(scales)
sceptre2_pc_dir <- "~/Documents/Projects/aux-sceptre2"
# load empirical distributions
resampling_dists <- readRDS(paste0(sceptre2_pc_dir, "/sceptre_resampling_dists_pc.rds"))
resampling_dists[1:5, 1:6]
q_list <- c(0.96)
samples_list <- 5e5
for (i in 1:length(q_list)) {
for (j in 1:1) {
# extract percentile and no_samples
q <- q_list[i]
samples <- samples_list[j]
# require to be in the low_moi_explore folder
# make a directory given the number of percentile and the number of resamples
working_dir <- getwd()
subDir <- sprintf("figures/power_exploration/tail_prob_%d_resamples_%.2f_percentile",  samples, q)
dir.create(file.path(working_dir, subDir))
# create gpd parameter matrix
gpd_param <- list(left = matrix(0, nrow = 7, ncol = nrow(resampling_dists)),
right = matrix(0, nrow = 7, ncol = nrow(resampling_dists)))
for (l in 1:nrow(resampling_dists)){
# extract index of resampling distribution
idx <- l
z_null <- as.numeric(resampling_dists[idx,2:(1+samples)])
plots <- list()
for(tail in c("left", "right")){
sign_flip <- if(tail == "right") 1 else -1
z_null_pos <- sign_flip*z_null
u_pos <- quantile(z_null_pos, q)
u <- sign_flip*u_pos
# extract the data
trun_data <- z_null_pos[z_null_pos > u_pos]
n <- sum(z_null_pos > u_pos)
# method of moment
xi_mom <- (1 - (mean(trun_data) - u_pos)^2/var(trun_data))/2
sig_mom <- (mean(trun_data) - u_pos)*(1 - xi_mom)
# first fit ks.test with estimated parameter; if there is Na, switch to simulation
test_ks <- ks.test(trun_data, "pgpd", u_pos, sig_mom, xi_mom)
print(is.nan(test_ks$p.value))
print(test_ks$p.value)
if(is.nan(test_ks$p.value)){
sim_data <- rgpd(1e6, loc = u_pos, scale = sig_mom, shape = xi_mom)
# k-s test
test_ks <- ks.test(trun_data, sim_data)
}
dgpd_density <- function(x) {
(1 - q) * dgpd(sign_flip*x,
loc = u_pos,
scale = sig_mom,
shape = xi_mom)
}
if(tail == "right"){
x_min <- u
x_max <- max(z_null)
} else{
x_min <- min(z_null)
x_max <- u
}
# hist_full <- tibble(z_null) %>%
#   ggplot(aes(x = z_null)) +
#   geom_histogram(aes(y = ..density..), bins = 1000) +
#   geom_vline(xintercept = u, linetype = "dashed", color = "red") +
#   stat_function(fun = dgpd_density,
#                 xlim = c(x_min, x_max),
#                 color = "dodgerblue")
#
# hist_tail <- hist_full +
#   coord_cartesian(xlim = c(x_min, x_max), ylim = c(0, dgpd_density(u)*1.5)) +
#   theme(plot.title = element_blank()) +
#   labs(title = sprintf("Stat = %0.4f, p = %0.1e", sqrt(test_ks$statistic), test_ks$p.value)) +
#   theme(plot.title = element_text(hjust = 0.5))
#
# expected_quantiles_pos <- qgpd(p = (2*(1:n)-1)/(2*n),
#                                loc = u_pos,
#                                scale = sig_mom,
#                                shape = xi_mom)
#
# expected_unif <- pgpd(q = sort(sign_flip*z_null_pos[z_null_pos > u_pos]),
#                       loc = u_pos,
#                       scale = sig_mom,
#                       shape = xi_mom)
#
# qq_plot <- tibble(expected_quantile = sort(sign_flip*expected_quantiles_pos),
#                   observed_quantile = sort(sign_flip*z_null_pos[z_null_pos > u_pos])) %>%
#   ggplot(aes(x = expected_quantile, y = observed_quantile)) +
#   geom_line() +
#   geom_abline(linetype = "dashed", color = "red")
#
# # plot the tail probability
observed_tail_prob <- (10:n)/n
observed_extreme_quantile <- quantile(trun_data, 1 - observed_tail_prob)
gpd_tail_prob <- 1 - as.vector(pgpd(q = observed_extreme_quantile,
loc = u_pos,
scale = sig_mom,
shape = xi_mom))
# # plot the tail probability
# p_plot <- tibble(observed_tail_prob = as.vector(observed_tail_prob),
#                  gpd_tail_prob = gpd_tail_prob,
#                  observed_prob_ub = observed_tail_prob + 1.96*sqrt(observed_tail_prob*(1-observed_tail_prob)/n),
#                  observed_prob_lb = observed_tail_prob - 1.96*sqrt(observed_tail_prob*(1-observed_tail_prob)/n)) %>%
#   ggplot(aes(x = gpd_tail_prob, y = observed_tail_prob)) +
#   geom_line() +
#   geom_ribbon(aes(ymin = observed_prob_lb, ymax = observed_prob_ub), alpha=0.0,
#               linetype = "dashed", colour="blue")+
#   geom_abline(linetype = "dashed", color = "red") +
#   scale_x_continuous(trans = c("log10", "reverse")) +
#   scale_y_continuous(trans = c("log10", "reverse"))
# store the plot
# plots[[tail]] <- plot_grid(hist_tail,
#                            hist_full,
#                            p_plot,
#                            ncol = 1,
#                            rel_heights = c(2,2,2.5, 2.5),
#                            align = "v")
# compute the ratio p-value
# ratio_pvalue <- max(gpd_tail_prob / observed_tail_prob,
#                     observed_tail_prob / gpd_tail_prob, na.rm = TRUE)
print(max(gpd_tail_prob / observed_tail_prob, na.rm = TRUE))
print(max(observed_tail_prob / gpd_tail_prob, na.rm = TRUE))
# save the gpd parameters and gof statistics and p-value
gpd_param[[which(names(gpd_param) == tail)]][1, l] <- u_pos
gpd_param[[which(names(gpd_param) == tail)]][2, l] <- sig_mom
gpd_param[[which(names(gpd_param) == tail)]][3, l] <- xi_mom
gpd_param[[which(names(gpd_param) == tail)]][4, l] <- test_ks$statistic
gpd_param[[which(names(gpd_param) == tail)]][5, l] <- test_ks$p.value
gpd_param[[which(names(gpd_param) == tail)]][6, l] <- max(gpd_tail_prob / observed_tail_prob, na.rm = TRUE)
gpd_param[[which(names(gpd_param) == tail)]][7, l] <- max(observed_tail_prob / gpd_tail_prob, na.rm = TRUE)
}
# plot
# final_plot <- plot_grid(plots[["left"]], plots[["right"]])
# save the plot in specific folder
# save_plot(sprintf("%s/%d.pdf", file.path(working_dir, subDir), idx), final_plot)
print(l)
}
# save the gpd parameter matrix and goodness of fit statistic and p-value
write.csv(gpd_param, sprintf("%s/param_twosides.csv", file.path(working_dir, subDir)))
}
}
library (readr)
url <- "https://raw.githubusercontent.com/Katsevich-Lab/sceptre2-manuscript/main/writeups/resampling_distributions/low_moi_explore/figures/power_exploration/sknorm_tail_prob_500000_resamples_0.96_percentile/param_twosides.csv"
mydata_sknorm <- read_csv(url(url))
knitr::opts_chunk$set(echo = TRUE)
library(eva)
library(tidyverse)
library(tidyverse)
library(katlabutils)
library(cowplot)
sceptre2_results_dir <- paste0(.get_config_path("LOCAL_SCEPTRE2_DATA_DIR"), "results/")
# load empirical distributions
resampling_dists <- readRDS(paste0(sceptre2_results_dir, "resampling_distributions/sceptre_resampling_dists.rds"))
resampling_dists[1:5, 1:6]
# mom fit
mom_fit <- function(y){
n <- length(y)
# maximum value gamma can take
max.gamma1 <- 0.5*(4-pi)*(2/(pi-2))^1.5 - (.Machine$double.eps)^(1/4)
# method of moments strategy
s <- sd(y)
gamma1 <- sum((y-mean(y))^3)/(n*s^3)
if(abs(gamma1) > max.gamma1) gamma1 <- sign(gamma1)*0.9*max.gamma1
cp1 <- as.numeric(c(mean(y), s, gamma1))
dp1 <- cp2dp(cp1, family="SN")
return(dp1)
}
library(eva)
library(tidyverse)
library(katlabutils)
library(cowplot)
# number of total sample
n <- 5e5
unique(resampling_dists[,3])
param_nc <- read_csv("figures/power_exploration/sknorm_tail_prob_500000_resamples_0.96_percentile/param_twosides.csv")
setwd("~/")
param_nc <- read_csv("figures/power_exploration/sknorm_tail_prob_500000_resamples_0.96_percentile/param_twosides.csv")
setwd("~/Documents/Projects/sceptre2/sceptre2-manuscript/writeups/resampling_distributions/low_moi_explore")
param_nc <- read_csv("figures/power_exploration/sknorm_tail_prob_500000_resamples_0.96_percentile/param_twosides.csv")
param_twosides <- t(param_nc[,-1])
overshoot_ratio <- as.numeric(param_twosides[, 6])
undershoot_ratio <- as.numeric(param_twosides[, 7])
quantile_list <- seq(0.01, 0.99, length.out = 10)
overshoot_set <- data.frame(index = numeric(10), ratio = numeric(10))
undershoot_set <- data.frame(index = numeric(10), ratio = numeric(10))
tail_list <- (10:(0.04*n)) / n
# find distributions based on right tail
for (r in 1:10){
dist <- abs(overshoot_ratio[331:660] - quantile(overshoot_ratio[331:660], quantile_list[r]))
overshoot_set[r, 1] <- which(dist == min(dist))
overshoot_set[r, 2] <- overshoot_ratio[which(dist == min(dist)) + 330]
dist <- abs(undershoot_ratio[331:660] - quantile(undershoot_ratio[331:660], quantile_list[r]))
undershoot_set[r, 1] <- which(dist == min(dist))
undershoot_set[r, 2] <- undershoot_ratio[which(dist == min(dist)) + 330]
}
undershoot_set
